{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Community Coronavirus\n",
    "\n",
    "Data Sources:\n",
    "\n",
    "1. [Our World In Data (Global)](https://ourworldindata.org/coronavirus)\n",
    "1. [Minister of Health Malaysia (Local)](https://github.com/MoH-Malaysia/covid19-public)\n",
    "\n",
    "The notebook cover two different perspective, the local view (Malaysia) and global view (World).<br>\n",
    "`Just COVID-19`<br>\n",
    "For local -> [Local view](#local_view)<br>\n",
    "& global -> [Global](#global_view)<br>\n",
    "\n",
    "Implemented:\n",
    "\n",
    "1. Simulating pandemic with SEIR compartment model. [Navigate](#seir_model)\n",
    "1. Using logistic function to identify inflection point and potential growth. [Navigate](#logistic_function)\n",
    "1. Exploratory data analysis.\n",
    "1. Forward Forecasting with ensemble machine learning model, stacked model and LSTM model. [Navigate](#predicting_section)\n",
    "\n",
    "[Dependencies](#dependencies)\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/9/94/Coronavirus._SARS-CoV-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Local<a id='local_view'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaF936sLYrXz"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLq83axgYrX0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from cycler import cycler\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from numba import jit\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from sklearn import *\n",
    "from sklearn import base\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "custom_style = {\n",
    "    'figure.autolayout': True,\n",
    "    'figure.titlesize': 20,\n",
    "    'figure.figsize': (10, 5),\n",
    "    'figure.dpi': 100,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.titlesize': 10,\n",
    "    'axes.titlelocation': 'left',\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.grid': True,\n",
    "    'axes.prop_cycle': cycler(\n",
    "        color=['#d73027', '#00518b', '#b1ef89', '#ffd500', '#000000']\n",
    "    ),\n",
    "    'grid.color': '#969696',\n",
    "    'xtick.direction': 'inout',\n",
    "    'ytick.direction': 'inout',\n",
    "    'xtick.minor.visible': True,\n",
    "    'ytick.minor.visible': True,\n",
    "    'ytick.right': True,\n",
    "    'ytick.left': False,\n",
    "    'ytick.labelright': True,\n",
    "    'ytick.labelleft': False,\n",
    "    'xaxis.labellocation': 'right',\n",
    "    'yaxis.labellocation': 'top',\n",
    "    'font.family': 'monospace',\n",
    "    'legend.fontsize': 10,\n",
    "    'legend.loc': 'best',\n",
    "}\n",
    "\n",
    "custom_style2 = {\n",
    "    'xtick.major.size': 7,\n",
    "    'xtick.minor.size': 3.5,\n",
    "    'xtick.major.width': 1.1,\n",
    "    'xtick.minor.width': 1.1,\n",
    "    'xtick.major.pad': 5,\n",
    "    'xtick.minor.visible': True,\n",
    "    'xtick.top': False,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.major.size': 7,\n",
    "    'ytick.minor.size': 3.5,\n",
    "    'ytick.major.width': 1.1,\n",
    "    'ytick.minor.width': 1.1,\n",
    "    'ytick.major.pad': 5,\n",
    "    'ytick.minor.visible': True,\n",
    "    'ytick.right': True,\n",
    "    'ytick.labelsize': 12,\n",
    "    'font.family': 'Palatino Linotype',\n",
    "    'font.size': 16,\n",
    "    'ytick.right': True,\n",
    "    'ytick.left': False,\n",
    "    'ytick.labelright': True,\n",
    "    'ytick.labelleft': False,\n",
    "    'xaxis.labellocation': 'right',\n",
    "    'yaxis.labellocation': 'top',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.left': False,\n",
    "}\n",
    "\n",
    "# %config InlineBackend.figure_format='svg'\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nap2LB7YrX1"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "owid = pd.read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n",
    "owid = owid[owid['location'] == 'Malaysia'][\n",
    "    [\n",
    "        'date',\n",
    "        'total_cases',\n",
    "        'new_cases',\n",
    "        'total_deaths',\n",
    "        'new_deaths',\n",
    "        'total_tests',\n",
    "        'new_tests',\n",
    "        'total_vaccinations',\n",
    "        'new_vaccinations',\n",
    "    ]\n",
    "]\n",
    "owid.columns = [\n",
    "    'date',\n",
    "    'total_cases',\n",
    "    'new_cases',\n",
    "    'total_deceased',\n",
    "    'new_deceased',\n",
    "    'total_tests',\n",
    "    'new_tests',\n",
    "    'total_vaccinations',\n",
    "    'new_vaccinations',\n",
    "]\n",
    "# owid = owid.iloc[70:-14]\n",
    "owid['date'] = pd.to_datetime(owid['date'])\n",
    "owid.set_index('date', inplace=True)\n",
    "assert type(owid.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "\n",
    "moh = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/MoH-Malaysia/covid19-public/main/epidemic/cases_malaysia.csv\"\n",
    ")\n",
    "moh = moh[['date', 'cases_recovered', 'cases_active']]\n",
    "moh.rename(\n",
    "    columns={'cases_recovered': 'new_recovered', 'cases_active': 'new_active'},\n",
    "    inplace=True,\n",
    ")\n",
    "moh['total_recovered'] = moh['new_recovered'].cumsum()\n",
    "moh['total_active'] = moh['new_active'].cumsum()\n",
    "moh['date'] = pd.to_datetime(moh['date'])\n",
    "moh.set_index('date', inplace=True)\n",
    "assert type(moh.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "\n",
    "data = pd.merge(owid, moh, how='left', left_index=True, right_index=True)\n",
    "data.columns = [\n",
    "    'total_cases',\n",
    "    'cases',\n",
    "    'total_deceased',\n",
    "    'deceased',\n",
    "    'total_tests',\n",
    "    'tests',\n",
    "    'total_vaccinations',\n",
    "    'vaccinations',\n",
    "    'recovered',\n",
    "    'active',\n",
    "    'total_recovered',\n",
    "    'total_active',\n",
    "]\n",
    "data = data.loc['2020-02-02':'2022-02-02']\n",
    "data.fillna(0, inplace=True)\n",
    "fdata = data.reset_index(drop=True)\n",
    "fdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNlBfy8aYrX5"
   },
   "source": [
    "### SEIR Model<a id='seir_model'></a>\n",
    "\n",
    "SEIR model, a compartment model for modelling infectious diseases spread where the total population is assigned to either Susceptible, Exposed, Infectious and Recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPZmw1m-YrX5"
   },
   "outputs": [],
   "source": [
    "skipped_window = 21\n",
    "total_infected = data['total_cases'][skipped_window:]\n",
    "total_deceased = data['total_deceased'][skipped_window:]\n",
    "total_recovered = data['total_recovered'][skipped_window:]\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def SEIR(t, y, beta, gamma, sigma, alpha, t_quarantine):\n",
    "\n",
    "    N = np.int(33e6 / (10 / 0.55))\n",
    "\n",
    "    S = y[0]  # Susceptible\n",
    "    E = y[1]  # Exposed\n",
    "    I = y[2]  # Infected\n",
    "    R = y[3]  # Recovered\n",
    "\n",
    "    if t > t_quarantine:\n",
    "        beta_t = beta * np.exp(-alpha * (t - t_quarantine))\n",
    "    else:\n",
    "        beta_t = beta\n",
    "\n",
    "    dS = -beta_t * S * I / N\n",
    "    dE = beta_t * S * I / N - sigma * E\n",
    "    dI = sigma * E - gamma * I\n",
    "    dR = gamma * I\n",
    "\n",
    "    return [dS, dE, dI, dR]\n",
    "\n",
    "\n",
    "def fitting_SEIR(vec, t_q, N, test_size):\n",
    "    beta, gamma, sigma, alpha = vec\n",
    "\n",
    "    t_f = total_infected.shape[0]\n",
    "    y0 = [N - total_infected[0], 0, total_infected[0], 0]\n",
    "    t_eval = np.arange(0, t_f, 1)\n",
    "    N = np.int(33e6 / (10 / 0.55))\n",
    "\n",
    "    sol = solve_ivp(\n",
    "        SEIR, [0, t_f], y0, args=(beta, gamma, sigma, alpha, t_q), t_eval=t_eval\n",
    "    )\n",
    "\n",
    "    split = np.int((1 - test_size) * total_infected.shape[0])\n",
    "\n",
    "    error = (\n",
    "        np.sum(\n",
    "            5\n",
    "            * (total_deceased[:split] + total_recovered[:split] - sol.y[3][:split]) ** 2\n",
    "        )\n",
    "        + np.sum(\n",
    "            (total_infected[:split] - np.cumsum(sol.y[1][:split] + sol.y[2][:split]))\n",
    "            ** 2\n",
    "        )\n",
    "    ) / split\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "def interpret_SEIR(plot=True):\n",
    "    N = np.int(33e6 / (10 / 0.55))\n",
    "    t_q = 21\n",
    "    t_f = total_infected.shape[0]\n",
    "    y0 = [N - total_infected[0], 0, total_infected[0], 0]\n",
    "    t_eval = np.arange(0, t_f, 1)\n",
    "    test_size = 0.1\n",
    "    opts = minimize(\n",
    "        fitting_SEIR, [2, 1, 0.8, 0.3], method='Nelder-Mead', args=(t_q, N, test_size)\n",
    "    )\n",
    "    beta, gamma, sigma, alpha = opts.x\n",
    "    sol = solve_ivp(\n",
    "        SEIR, [0, t_f], y0, args=(beta, gamma, sigma, alpha, t_q), t_eval=t_eval\n",
    "    )\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "        ax.plot(\n",
    "            data.index[skipped_window:],\n",
    "            np.cumsum(sol.y[1] + sol.y[2]),\n",
    "            label='Exposed and Infected',\n",
    "        )\n",
    "        ax.plot(data.index[skipped_window:], total_infected, label='Infected')\n",
    "        ax.plot(data.index[skipped_window:], sol.y[3], label='Recovered')\n",
    "        plt.suptitle(\n",
    "            'SEIR Model: An Assumption', ha='left', x=0.015, y=0.95, fontsize=20\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Population')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.title(\n",
    "            'Skipped Window: {} | Quarantine Take Place {} days after outbreak'.format(\n",
    "                skipped_window, t_q\n",
    "            )\n",
    "        )\n",
    "        plt.savefig(\"fig/seir_model.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "interpret_SEIR(plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE1YDYPaYrX6"
   },
   "source": [
    "### Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3RI3T1-YrX6"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def percent_change(df, target_columns):\n",
    "    assert type(df) == pd.core.frame.DataFrame\n",
    "    assert type(target_columns) == str\n",
    "    percent_change = df[target_columns].pct_change() * 100\n",
    "    return percent_change\n",
    "\n",
    "\n",
    "def absolute_change(df, target_columns):\n",
    "    assert type(df) == pd.core.frame.DataFrame\n",
    "    assert type(target_columns) == str\n",
    "    absolute_change = df[target_columns] - df[target_columns].shift(1)\n",
    "    return absolute_change\n",
    "\n",
    "\n",
    "def style_negative(v, props=\"\"):\n",
    "    return props if v < 0 else None\n",
    "\n",
    "\n",
    "def style_positive(v, props=\"\"):\n",
    "    return props if v > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1640180956779,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "6DZiseWtYrX7",
    "outputId": "6d5d3958-b9ea-45bc-db1d-0fd0bcf2add4"
   },
   "outputs": [],
   "source": [
    "def changes_in_series(df):\n",
    "    data = df.copy()\n",
    "    print(\"Stats In a Glance\")\n",
    "    needed = ['tests', 'cases', 'deceased', 'vaccinations', 'recovered']\n",
    "    for c in needed:\n",
    "        data[c + ' Percent Change'] = percent_change(data, c)\n",
    "\n",
    "    percent_change_cols = [col for col in data.columns if 'Percent Change' in col]\n",
    "    display(\n",
    "        data[percent_change_cols]\n",
    "        .tail(20)\n",
    "        .style.applymap(style_negative, props='color:red;')\n",
    "        .applymap(style_positive, props='color:green;')\n",
    "        .format(na_rep='Missing')\n",
    "    )\n",
    "\n",
    "\n",
    "changes_in_series(df=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jJUL0VgYrX7"
   },
   "source": [
    "### Logistic Function<a id='logistic_function'></a>\n",
    "\n",
    "Additional to SEIR general mathematic model, Logistic Function can be also used to estimate the inflection point and the overall trend of the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygNp4JUEYrX7"
   },
   "outputs": [],
   "source": [
    "def logistic_function(x, k, x_0, maxy):\n",
    "    return maxy / (\n",
    "        1 + np.exp(-k * (x - x_0))\n",
    "    )  # k = growth rate | x_0 = inflection point | maxy = the curve maximum value\n",
    "\n",
    "\n",
    "def logistic_pred(df, cn, initial_start, plot=True):\n",
    "    assert type(cn) == str\n",
    "\n",
    "    truey = df[cn][initial_start:]\n",
    "    data_length = range(truey.shape[0])\n",
    "    popt, pcov = curve_fit(\n",
    "        logistic_function, data_length, truey, bounds=([0, 0, 0], np.inf), maxfev=2000\n",
    "    )\n",
    "    estimated_k, estimated_x_0, maxy = popt\n",
    "    predy = logistic_function(data_length, estimated_k, estimated_x_0, maxy)\n",
    "    mse = np.square(np.subtract(truey, predy)).mean()\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "        ax.plot(data_length, predy, 'r--', label='Predict', linewidth=2)\n",
    "        ax.plot(data_length, truey, color='black', label='Actual', linewidth=2)\n",
    "        ax.legend(fontsize='large')\n",
    "        ax.set_xlabel(\n",
    "            'Logistic Function Error vs Actual | MSE: {}'.format(round(mse, 4)),\n",
    "            fontsize=10,\n",
    "        )\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.suptitle(\n",
    "            'Logistic Function on Inflection Point & Growth Rate',\n",
    "            ha='left',\n",
    "            x=0.015,\n",
    "            y=0.95,\n",
    "            fontsize=20,\n",
    "        )\n",
    "        ax.set_title(\n",
    "            'Estimated Growth Rate: {} | Estimated Inflection Point: {} | Estimated Maximum Cases: {}'.format(\n",
    "                round(estimated_k, 4), round(estimated_x_0, 4), round(maxy, 4)\n",
    "            )\n",
    "        )\n",
    "        plt.savefig(\"fig/logistic_growth.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "logistic_pred(df=fdata, cn='cases', initial_start=0, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLVX4_WeYrX8"
   },
   "outputs": [],
   "source": [
    "def condition_color_list(columns):\n",
    "    color_list = []\n",
    "    for v in columns:\n",
    "        if v > 0:\n",
    "            color_list.append('#15607a')\n",
    "        else:\n",
    "            color_list.append('#ff483a')\n",
    "    return color_list\n",
    "\n",
    "\n",
    "def plot_pc(df):\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    with plt.style.context(custom_style2):\n",
    "        plt.style.use(['dark_background'])\n",
    "        temp = df.copy()\n",
    "        temp['Total Confirm Percent Change'] = percent_change(temp, 'cases')\n",
    "\n",
    "        color_list = condition_color_list(temp['Total Confirm Percent Change'])\n",
    "        fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "        plt.tight_layout()\n",
    "        ax.bar(\n",
    "            x=temp.index, height=temp['Total Confirm Percent Change'], color=color_list\n",
    "        )\n",
    "        ax.plot(\n",
    "            temp.index,\n",
    "            [np.mean(temp['Total Confirm Percent Change'])] * len(temp.index),\n",
    "            label='Mean',\n",
    "            linestyle='--',\n",
    "            color='#ffb55f',\n",
    "        )\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        ax.grid(axis='y')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Percent Change')\n",
    "        ax.set_title('Covid-19 Confirmed Percent Change', fontweight='bold', loc='left')\n",
    "        ax.set_ylim([-100, 500])\n",
    "        print(\n",
    "            'Max: {}, Min: {}'.format(\n",
    "                temp['Total Confirm Percent Change'].max(),\n",
    "                temp['Total Confirm Percent Change'].min(),\n",
    "            )\n",
    "        )\n",
    "        plt.savefig(\"fig/percent_change.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_pc(df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO2-71-SYrX8"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4QO47saYrX-"
   },
   "outputs": [],
   "source": [
    "def plot_decomposition(df, cn, window, plot=True):\n",
    "    assert type(cn) == str\n",
    "\n",
    "    values = df[cn].values.flatten()\n",
    "    rm = np.convolve(values, np.ones(window) / window, mode='valid')\n",
    "    detrended = values[window - 1 :] / rm\n",
    "    diff = df[cn] - df[cn].shift(1)\n",
    "\n",
    "    new = df.reset_index().copy()\n",
    "    new['day'] = new['date'].dt.day_name()\n",
    "    countday = new.groupby('day', as_index=False)[[cn]].sum()\n",
    "\n",
    "    nh = df.copy()\n",
    "    nh['cummax'] = nh[cn].cummax()\n",
    "    nh = nh.drop_duplicates(subset='cummax', keep='first').reset_index()\n",
    "    nh['diff'] = (nh['date'].diff() / np.timedelta64(1, 'D')).fillna(0)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=True, dpi=200)\n",
    "        index = df.index\n",
    "        axs[0].plot(index, values)\n",
    "        axs[0].set_title('Abstract')\n",
    "        axs[0].set_ylabel('Daily')\n",
    "        axs[0].plot(index[window - 1 :], rm)\n",
    "        axs[1].plot(index, diff)\n",
    "        axs[1].set_title('Disparity')\n",
    "        axs[1].set_ylabel('Daily')\n",
    "        axs[2].plot(index[window - 1 :], detrended)\n",
    "        axs[2].set_title('Detrended')\n",
    "        for i in range(3):\n",
    "            axs[i].yaxis.set_label_position('right')\n",
    "        plt.suptitle('Decomposition', ha='left', x=0.015, y=1)\n",
    "        fig.autofmt_xdate()\n",
    "        plt.savefig(\"fig/eda_1.png\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10, 8), dpi=200)\n",
    "        axs[0].bar(x=countday['day'], height=countday[cn])\n",
    "        axs[0].set_title('Categorize')\n",
    "        axs[0].set_ylabel('Total')\n",
    "        axs[1].hist(x=new[cn], bins=10)\n",
    "        axs[1].set_title('Distributions')\n",
    "        axs[1].set_ylabel('Counts')\n",
    "        for i in range(2):\n",
    "            axs[i].yaxis.set_label_position('right')\n",
    "        plt.savefig(\"fig/eda_2.png\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5), dpi=200)\n",
    "        ax.plot(nh['date'], nh['cummax'])\n",
    "        for p in range(len(nh)):\n",
    "            if nh['diff'][p] <= 5.0:\n",
    "                pass\n",
    "            else:\n",
    "                y_ratio = nh['cummax'].max() / 275\n",
    "                ax.annotate(\n",
    "                    str(nh['diff'][p]),\n",
    "                    (mdates.date2num(nh['date'][p]), p),\n",
    "                    xytext=(-20, nh['cummax'][p] / y_ratio),\n",
    "                    textcoords='offset pixels',\n",
    "                )\n",
    "        ax.set_title('Interval')\n",
    "        ax.set_ylabel('Cumulative Max')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.savefig(\"fig/eda_3.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_decomposition(df=data, cn='cases', window=12, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Local<a id='predicting_section'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaJ-vOjVYrX_"
   },
   "source": [
    "### Model Selection and Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1647,
     "status": "ok",
     "timestamp": 1640180981457,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "jgPGSm08YrX_",
    "outputId": "67f29a6a-0870-4705-f376-8a20c68b50d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(df, cn):\n",
    "    assert type(cn) == str\n",
    "\n",
    "    # from sklearn import linear_model, preprocessing, model_selection, pipeline, ensemble, tree, kernel_ridge, neighbors, base\n",
    "\n",
    "    X = df.drop(columns=cn)\n",
    "    y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(\n",
    "        X, y, test_size=0.8, random_state=7\n",
    "    )\n",
    "\n",
    "    lasso_grid = model_selection.GridSearchCV(\n",
    "        linear_model.Lasso(),\n",
    "        param_grid={\n",
    "            'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'normalize': [True, False],\n",
    "            'tol': [0.001, 0.01, 0.1, 1],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    eas_grid = model_selection.GridSearchCV(\n",
    "        linear_model.ElasticNet(),\n",
    "        param_grid={\n",
    "            'alpha': [1.0, 0.01, 0.1, 0.5, 1],\n",
    "            'l1_ratio': [0.01, 0.5, 0.1, 1],\n",
    "            'normalize': [True, False],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    gbr_grid = model_selection.GridSearchCV(\n",
    "        ensemble.GradientBoostingRegressor(),\n",
    "        param_grid={\n",
    "            'learning_rate': [0.1, 0.5, 1],\n",
    "            'n_estimators': [100, 110, 120],\n",
    "            'min_samples_split': [2, 4, 8],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    rfr_grid = model_selection.GridSearchCV(\n",
    "        ensemble.RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': [110, 120],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [1],\n",
    "            'max_depth': [6, 9],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    lgb_grid = model_selection.GridSearchCV(\n",
    "        lgb.LGBMRegressor(),\n",
    "        param_grid={\n",
    "            'num_leaves': [10, 31, 50, 75, 150],\n",
    "            'max_depth': [25, 50, 75],\n",
    "            'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "            'n_estimators': [100, 125, 150],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    xgb_grid = model_selection.GridSearchCV(\n",
    "        xgb.XGBRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': [100, 125, 150],\n",
    "            'max_depth': [25, 50, 75],\n",
    "            'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    ridge_grid = model_selection.GridSearchCV(\n",
    "        linear_model.Ridge(),\n",
    "        param_grid={\n",
    "            'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "            'normalize': [True, False],\n",
    "            'tol': [0.001, 0.01, 0.1, 1],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    sgd_grid = model_selection.GridSearchCV(\n",
    "        linear_model.SGDRegressor(),\n",
    "        param_grid={\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "            'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "            'tol': [0.001, 0.01, 0.1, 1],\n",
    "            'early_stopping': [True],\n",
    "            'warm_start': [True, False],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    dt_grid = model_selection.GridSearchCV(\n",
    "        tree.DecisionTreeRegressor(),\n",
    "        param_grid={\n",
    "            'min_samples_split': [2, 4, 5],\n",
    "            'min_samples_leaf': [1, 2, 3],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_leaf_nodes': [10, 15, 20, 30],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    ada_grid = model_selection.GridSearchCV(\n",
    "        ensemble.AdaBoostRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': [25, 50, 75, 100],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'loss': ['linear', 'square', 'exponential'],\n",
    "            'random_state': [7],\n",
    "        },\n",
    "        cv=10,\n",
    "        scoring='neg_mean_squared_error',\n",
    "    )\n",
    "\n",
    "    kerid_grid = model_selection.GridSearchCV(\n",
    "        kernel_ridge.KernelRidge(),\n",
    "        param_grid={'alpha': [0.01, 0.1, 1], 'gamma': [0.001, 0.01, 1]},\n",
    "    )\n",
    "\n",
    "    kn_grid = model_selection.GridSearchCV(\n",
    "        neighbors.KNeighborsRegressor(),\n",
    "        param_grid={\n",
    "            'n_neighbors': [5, 10, 15],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'leaf_size': [5, 10, 20, 30, 40, 50],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # for m in [lasso_grid, eas_grid, gbr_grid, rfr_grid, lgb_grid, xgb_grid,\n",
    "    #           ridge_grid, sgd_grid, dt_grid, ada_grid, kerid_grid, kn_grid]:\n",
    "    #     m.fit(trainX, trainy)\n",
    "    #     print(m.best_params_)\n",
    "\n",
    "    # Best params\n",
    "    lasrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Lasso(\n",
    "            alpha=0.0001,\n",
    "            max_iter=10000,\n",
    "            normalize=False,\n",
    "            tol=0.001,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    easrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.ElasticNet(\n",
    "            alpha=1.0, l1_ratio=0.01, normalize=False, warm_start=True, random_state=7\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gbrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        ensemble.GradientBoostingRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    rfrss = pipeline.make_pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        ensemble.RandomForestRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    lgbmm = pipeline.make_pipeline(\n",
    "        preprocessing.MinMaxScaler(),\n",
    "        lgb.LGBMRegressor(\n",
    "            learning_rate=0.1,\n",
    "            max_depth=25,\n",
    "            n_estimators=100,\n",
    "            num_leaves=10,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    xgbrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        xgb.XGBRegressor(\n",
    "            learning_rate=0.1,\n",
    "            max_depth=25,\n",
    "            min_child_weight=3,\n",
    "            n_estimators=100,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ridrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7),\n",
    "    )\n",
    "\n",
    "    sgdrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.SGDRegressor(\n",
    "            alpha=0.1,\n",
    "            early_stopping=True,\n",
    "            max_iter=1000,\n",
    "            penalty='elasticnet',\n",
    "            tol=0.001,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dtrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        tree.DecisionTreeRegressor(\n",
    "            max_features='sqrt',\n",
    "            max_leaf_nodes=20,\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    adars = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        ensemble.AdaBoostRegressor(\n",
    "            learning_rate=1, loss='square', n_estimators=25, random_state=7\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    kerrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(), kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001)\n",
    "    )\n",
    "\n",
    "    knrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'),\n",
    "    )\n",
    "\n",
    "    model_namelist = [\n",
    "        'Lasso',\n",
    "        'ElasticNet',\n",
    "        'GradientBoosting',\n",
    "        'RandomForest',\n",
    "        'LightGBM',\n",
    "        'XGBoost',\n",
    "        'Ridge',\n",
    "        'SGD',\n",
    "        'DecisionTree',\n",
    "        'AdaBoost',\n",
    "        'KernelRidge',\n",
    "        'KNeighbors',\n",
    "    ]\n",
    "\n",
    "    model_list = [\n",
    "        lasrs,\n",
    "        easrs,\n",
    "        gbrrs,\n",
    "        rfrss,\n",
    "        lgbmm,\n",
    "        xgbrs,\n",
    "        ridrs,\n",
    "        sgdrs,\n",
    "        dtrrs,\n",
    "        adars,\n",
    "        kerrs,\n",
    "        knrrs,\n",
    "    ]\n",
    "    for n, m in zip(model_namelist, model_list):\n",
    "        m.fit(trainX, trainy)\n",
    "        predicted = m.predict(testX)\n",
    "        mse = np.mean((testy - predicted) ** 2)\n",
    "        print(\"Model: {0:20} MSE: {1}\".format(n, round(mse, 2)))\n",
    "\n",
    "\n",
    "hyperparameter_tuning(df=fdata, cn='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1640180984051,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "HPC3jXlqYrYB",
    "outputId": "b298270a-b529-4ba4-fe03-80908946c136"
   },
   "outputs": [],
   "source": [
    "def ml_feature_selection(df, cn, tnf):\n",
    "    assert type(cn) == str\n",
    "\n",
    "    X = df.drop(columns=cn)\n",
    "    y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(\n",
    "        X, y, test_size=0.8, random_state=7\n",
    "    )\n",
    "\n",
    "    randomforest = ensemble.RandomForestRegressor(\n",
    "        max_depth=6,\n",
    "        max_features='log2',\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=2,\n",
    "        n_estimators=110,\n",
    "        warm_start=True,\n",
    "        random_state=7,\n",
    "    )\n",
    "    randomforest.fit(trainX, trainy)\n",
    "    fi = randomforest.feature_importances_\n",
    "    indices = np.argsort(fi)\n",
    "    columns = trainX.columns\n",
    "    std = np.std([tree.feature_importances_ for tree in randomforest.estimators_])\n",
    "    print(\n",
    "        'Explonatory Variables: {} \\nResponse Variables: {}\\n'.format(\n",
    "            trainX.columns.tolist(), cn\n",
    "        )\n",
    "    )\n",
    "    print('Feature in Order: ')\n",
    "    for f in range(trainX.shape[1]):\n",
    "        print(\n",
    "            '%d. Feature_no: %d Feature_name: %s (%f)'\n",
    "            % (f + 1, indices[f], columns[indices[f]], fi[indices[f]])\n",
    "        )\n",
    "\n",
    "    feature_selected = columns[indices].tolist()\n",
    "    feature_selected.insert(0, cn)\n",
    "    return feature_selected\n",
    "\n",
    "\n",
    "feature_selected = ml_feature_selection(df=fdata, cn='cases', tnf=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQAWUnb2YrYB"
   },
   "source": [
    "### Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2618,
     "status": "ok",
     "timestamp": 1640180989244,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "pHbE9AIGYrYC",
    "outputId": "71ce2177-e661-4cdc-8726-71b023e20080"
   },
   "outputs": [],
   "source": [
    "class AveragingModels(base.BaseEstimator, base.RegressorMixin, base.TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [base.clone(x) for x in self.models]\n",
    "\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "\n",
    "def CombinedModels(df, cn):\n",
    "    assert type(cn) == str\n",
    "\n",
    "    lasrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Lasso(\n",
    "            alpha=0.0001,\n",
    "            max_iter=10000,\n",
    "            normalize=False,\n",
    "            tol=0.001,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gbrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        ensemble.GradientBoostingRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    rfrss = pipeline.make_pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        ensemble.RandomForestRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ridrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7),\n",
    "    )\n",
    "\n",
    "    knrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'),\n",
    "    )\n",
    "\n",
    "    kerrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(), kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001)\n",
    "    )\n",
    "\n",
    "    model_list = [lasrs, gbrrs, rfrss]\n",
    "\n",
    "    X = df.drop(columns=cn)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(\n",
    "        X, y, test_size=0.8, random_state=7\n",
    "    )\n",
    "\n",
    "    for m in model_list:\n",
    "        m.fit(trainX, trainy)\n",
    "        pred = m.predict(testX)\n",
    "\n",
    "    # Stack Model\n",
    "    stacked = StackingCVRegressor(\n",
    "        regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs),\n",
    "        meta_regressor=lasrs,\n",
    "        use_features_in_secondary=True,\n",
    "        random_state=7,\n",
    "    )\n",
    "    stacked.fit(trainX, trainy)\n",
    "    stacked_predicted = stacked.predict(testX)\n",
    "    stacked_mse = np.square(np.subtract(testy, stacked_predicted)).mean()\n",
    "    print('Stacked MSE:', round(stacked_mse, 2))\n",
    "\n",
    "    # Blended Model\n",
    "    blended_model = (\n",
    "        (0.2 * model_list[0].predict(testX))\n",
    "        + (0.40 * model_list[1].predict(testX))\n",
    "        + (0.40 * model_list[2].predict(testX))\n",
    "    )\n",
    "\n",
    "    blended_mse = np.square(np.subtract(testy, blended_model)).mean()\n",
    "    print('Blended MSE:', round(blended_mse, 2))\n",
    "\n",
    "    # Average Model\n",
    "    averaged_models = AveragingModels(models=(model_list))\n",
    "    averaged_models.fit(trainX, trainy)\n",
    "    averaged_predicted = averaged_models.predict(testX)\n",
    "    ageraged_mse = np.square(np.subtract(testy, averaged_predicted)).mean()\n",
    "    print('Averaged MSE:', round(ageraged_mse, 2))\n",
    "\n",
    "    return stacked\n",
    "\n",
    "\n",
    "stacked_model = CombinedModels(df=fdata, cn=\"cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O3aObx3YrYC"
   },
   "source": [
    "Average Model: Lasso, GradientBoosting, RandomForest <br>\n",
    "Stacked Model: Lasso, GradientBoosting, RandomForest, Ridge, Kneighbors, KernelRidge <br>\n",
    "Blended Model: 0.2(Lasso), 0.4(GradientBoosting), 0.4(RandomForest) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0nGSwiyYrYD"
   },
   "source": [
    "### Forecast with Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBMp3CsDYrYD"
   },
   "outputs": [],
   "source": [
    "def lasso_model(df, cn, ntf, lag, forecast, plot):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Timeseries to supervised and forecast Lasso\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The dataframe with explanatory and response variables \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        ntf: int\n",
    "            The number of top feature to select rank by lasso coefficient\n",
    "        lag: int\n",
    "            The preceding lag for time series data construction\n",
    "        forecast: integer\n",
    "            The time horizon to forecast in days\n",
    "        plot: boolean, optional\n",
    "            Visualize plot\n",
    "    -----------\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    trainX = df.sample(frac=0.8, random_state=7)\n",
    "    testX = df.drop(trainX.index)\n",
    "    trainy, testy = trainX.pop(cn), testX.pop(cn)\n",
    "\n",
    "    l = linear_model.Lasso(\n",
    "        alpha=0.0001,\n",
    "        max_iter=10000,\n",
    "        normalize=False,\n",
    "        tol=0.001,\n",
    "        warm_start=True,\n",
    "        random_state=7,\n",
    "    )\n",
    "    l.fit(trainX, trainy)\n",
    "    pred = l.predict(testX)\n",
    "    feature_df = pd.DataFrame(\n",
    "        {'Feature': df.drop(columns=cn).columns, 'Coefficient': l.coef_}\n",
    "    )\n",
    "    tf = (\n",
    "        feature_df.sort_values(by='Coefficient', ascending=False)\n",
    "        .iloc[:ntf, 0]\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    # Feature selection\n",
    "    lasso = linear_model.Lasso(\n",
    "        alpha=0.0001,\n",
    "        max_iter=10000,\n",
    "        normalize=False,\n",
    "        tol=0.001,\n",
    "        warm_start=True,\n",
    "        random_state=7,\n",
    "    )\n",
    "    lasso.fit(trainX[tf], trainy)\n",
    "    apred = lasso.predict(testX[tf])\n",
    "\n",
    "    # Training\n",
    "    s_feature_model = list()\n",
    "    for x in range(len(tf)):\n",
    "        data = df[tf[x]].tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        temp = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "\n",
    "        for i in range(lag, 0, -1):  # Input sequence (t-n, ... t-1)\n",
    "            cols.append(temp.shift(i))\n",
    "            names += [('%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        cols.append(temp.shift(-1))  # Current forecast t\n",
    "        names += [('%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "\n",
    "        new_df = pd.concat(cols, axis=1)\n",
    "        new_df.columns = names\n",
    "        new_df.dropna(inplace=True)\n",
    "        new_df = new_df.add_prefix(str(tf[x]) + ' ')\n",
    "\n",
    "        XX, yy = new_df.iloc[:, :lag], new_df.iloc[:, lag : lag + 1]\n",
    "        trainXX = XX.sample(frac=0.8, random_state=7)\n",
    "        testXX = XX.drop(trainXX.index)\n",
    "        trainyy = yy.sample(frac=0.8, random_state=7)\n",
    "        testyy = yy.drop(trainyy.index)\n",
    "        model = linear_model.Lasso(\n",
    "            alpha=0.0001,\n",
    "            max_iter=10000,\n",
    "            normalize=False,\n",
    "            tol=0.001,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        )\n",
    "        model.fit(trainXX, trainyy)\n",
    "        s_feature_model.append(model)\n",
    "        # print(np.sqrt(np.mean(testyy.values.flatten()-model.predict(testXX))**2))\n",
    "\n",
    "    # Forecast\n",
    "    result = df[tf].iloc[-lag:, :].copy()\n",
    "    # display(result)\n",
    "    result['Forecast'] = 0\n",
    "    for _ in range(forecast):\n",
    "        temp_result = list()\n",
    "        for x in range(len(s_feature_model)):\n",
    "            temp_result.append(\n",
    "                int(\n",
    "                    s_feature_model[x].predict(\n",
    "                        result.iloc[-lag:, x].values.reshape(1, -1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        temp_result.append(int(lasso.predict(np.array(temp_result).reshape(1, -1))))\n",
    "        result = result.append(\n",
    "            pd.Series(temp_result, index=result.columns), ignore_index=True\n",
    "        )\n",
    "\n",
    "    moving_average_window = 7\n",
    "    result['Forecast Moving Average'] = (\n",
    "        result['Forecast'].rolling(window=moving_average_window).mean()\n",
    "    )\n",
    "\n",
    "    # Output\n",
    "    # feature_df.plot.bar(x='Feature', y='Coefficient')\n",
    "    # print('Without feature Selection: {}'.format(np.sqrt(np.mean(testy-pred)**2)))\n",
    "    # print('With feature selection: {}'.format(np.sqrt(np.mean(testy-apred)**2)))\n",
    "    # print('Forecast: {} | Lag: {}'.format(forecast, lag))\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(dpi=200)\n",
    "        plt.plot(df[cn])\n",
    "        plt.plot(\n",
    "            pd.Series(\n",
    "                result['Forecast'][-forecast:].tolist(),\n",
    "                index=np.arange(\n",
    "                    int(df.index[-1:].values[0]),\n",
    "                    int(df.index[-1:].values[0]) + forecast,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        plt.plot(\n",
    "            pd.Series(\n",
    "                result['Forecast Moving Average'][\n",
    "                    -forecast + moving_average_window :\n",
    "                ].tolist(),\n",
    "                index=np.arange(\n",
    "                    int(df.index[-1:].values[0]) + moving_average_window,\n",
    "                    int(df.index[-1:].values[0]) + forecast,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        plt.legend(\n",
    "            ['Actual', 'Forecast', 'Forecast Moving Average'], ncol=5, loc='upper left'\n",
    "        )\n",
    "        plt.title(\n",
    "            'Feature Selected: {} | Forecast: {} | Lag: {}'.format(ntf, forecast, lag),\n",
    "            color='#787878',\n",
    "        )\n",
    "        plt.suptitle(\n",
    "            'Projection: {} Forward Forecast'.format(cn),\n",
    "            ha='left',\n",
    "            x=0.015,\n",
    "            y=0.95,\n",
    "            fontsize=20,\n",
    "        )\n",
    "        plt.figtext(0.85, 0.9, 'Lasso Model', ha='left')\n",
    "        ax.set_xlabel('Range')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 2127,
     "status": "ok",
     "timestamp": 1640181075626,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "7kXR_pVfYrYD",
    "outputId": "cea2293e-debb-43b2-a40f-50cb93700878"
   },
   "outputs": [],
   "source": [
    "# lasso_model(df=fdata, cn='cases', ntf=3, lag=7, forecast=90, plot=True)\n",
    "# lasso_model(df=fdata, cn='cases', ntf=3, lag=30, forecast=90, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOt5FhVnYrYE"
   },
   "source": [
    "### Forecast with Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHvWvIGoYrYE"
   },
   "outputs": [],
   "source": [
    "\"def stacked_model(df, cn, lag, forecast, plot):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Timeseries to supervised and forecast Lasso\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The dataframe with explanatory and response variables \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        lag: int\n",
    "            The preceding lag for time series data construction\n",
    "        forecast: integer\n",
    "            The time horizon to forecast in days\n",
    "        plot: boolean, optional\n",
    "            Visualize plot\n",
    "    -----------\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    trainX = df.sample(frac=0.8, random_state=7)\n",
    "    testX = df.drop(trainX.index)\n",
    "    trainy, testy = trainX.pop(cn), testX.pop(cn)\n",
    "    tf = trainX.columns.tolist()\n",
    "\n",
    "    lasrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Lasso(\n",
    "            alpha=0.0001,\n",
    "            max_iter=10000,\n",
    "            normalize=False,\n",
    "            tol=0.001,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gbrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        ensemble.GradientBoostingRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    rfrss = pipeline.make_pipeline(\n",
    "        preprocessing.StandardScaler(),\n",
    "        ensemble.RandomForestRegressor(\n",
    "            max_depth=6,\n",
    "            max_features='log2',\n",
    "            min_samples_leaf=1,\n",
    "            min_samples_split=2,\n",
    "            n_estimators=110,\n",
    "            warm_start=True,\n",
    "            random_state=7,\n",
    "        ),\n",
    "    )\n",
    "    ridrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7),\n",
    "    )\n",
    "\n",
    "    knrrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(),\n",
    "        neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'),\n",
    "    )\n",
    "\n",
    "    kerrs = pipeline.make_pipeline(\n",
    "        preprocessing.RobustScaler(), kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001)\n",
    "    )\n",
    "\n",
    "    stacked_model = StackingCVRegressor(\n",
    "        regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs),\n",
    "        meta_regressor=lasrs,\n",
    "        use_features_in_secondary=True,\n",
    "        random_state=7,\n",
    "    )\n",
    "\n",
    "    stacked_model.fit(trainX, trainy)\n",
    "    pred = stacked_model.predict(testX)\n",
    "\n",
    "    # Training\n",
    "    model_list = list()\n",
    "    for x in range(len(tf)):\n",
    "        data = df[tf[x]].tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        temp = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "\n",
    "        for i in range(lag, 0, -1):  # Input sequence (t-n, ... t-1)\n",
    "            cols.append(temp.shift(i))\n",
    "            names += [('%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "        cols.append(temp.shift(-1))  # Current forecast t\n",
    "        names += [('%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "\n",
    "        new_df = pd.concat(cols, axis=1)\n",
    "        new_df.columns = names\n",
    "        new_df.dropna(inplace=True)\n",
    "        new_df = new_df.add_prefix(str(tf[x]) + ' ')\n",
    "\n",
    "        XX, yy = new_df.iloc[:, :lag], new_df.iloc[:, lag : lag + 1]\n",
    "        trainXX = XX.sample(frac=0.8, random_state=7)\n",
    "        testXX = XX.drop(trainXX.index)\n",
    "        trainyy = yy.sample(frac=0.8, random_state=7)\n",
    "        testyy = yy.drop(trainyy.index)\n",
    "        model = StackingCVRegressor(\n",
    "            regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs),\n",
    "            meta_regressor=lasrs,\n",
    "            use_features_in_secondary=True,\n",
    "            random_state=7,\n",
    "        )\n",
    "        model.fit(trainXX, trainyy)\n",
    "        model_list.append(model)\n",
    "        # print(np.sqrt(np.mean(testyy.values.flatten()-model.predict(testXX))**2))\n",
    "\n",
    "    # Forecast\n",
    "    result = df[tf].iloc[-lag:, :].copy()\n",
    "    # display(result)\n",
    "    result['Forecast'] = 0\n",
    "    for _ in range(forecast):\n",
    "        temp_result = list()\n",
    "        for x in range(len(model_list)):\n",
    "            temp_result.append(\n",
    "                int(model_list[x].predict(result.iloc[-lag:, x].values.reshape(1, -1)))\n",
    "            )\n",
    "        temp_result.append(\n",
    "            int(stacked_model.predict(np.array(temp_result).reshape(1, -1)))\n",
    "        )\n",
    "        result = result.append(\n",
    "            pd.Series(temp_result, index=result.columns), ignore_index=True\n",
    "        )\n",
    "\n",
    "    moving_average_window = 7\n",
    "    result['Forecast Moving Average'] = (\n",
    "        result['Forecast'].rolling(window=moving_average_window).mean()\n",
    "    )\n",
    "\n",
    "    # Output\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(dpi=200)\n",
    "        plt.plot(df[cn])\n",
    "        plt.plot(\n",
    "            pd.Series(\n",
    "                result['Forecast'][-forecast:].tolist(),\n",
    "                index=np.arange(\n",
    "                    int(df.index[-1:].values[0]),\n",
    "                    int(df.index[-1:].values[0]) + forecast,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        plt.plot(\n",
    "            pd.Series(\n",
    "                result['Forecast Moving Average'][\n",
    "                    -forecast + moving_average_window :\n",
    "                ].tolist(),\n",
    "                index=np.arange(\n",
    "                    int(df.index[-1:].values[0]) + moving_average_window,\n",
    "                    int(df.index[-1:].values[0]) + forecast,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        plt.legend(\n",
    "            ['Actual', 'Forecast', 'Forecast Moving Average'], ncol=5, loc='upper left'\n",
    "        )\n",
    "        plt.title(\"Forecast: {} | Lag: {}\".format(forecast, lag), color='#787878')\n",
    "        plt.suptitle(\n",
    "            'Projection: {} Forward Forecast'.format(cn),\n",
    "            ha='left',\n",
    "            x=0.015,\n",
    "            y=0.95,\n",
    "            fontsize=20,\n",
    "        )\n",
    "        plt.figtext(0.85, 0.9, 'Stacked Model', ha='left')\n",
    "        ax.set_xlabel('Range')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.savefig(\n",
    "            \"fig/stacked_model_local_{}_{}_{}_{}\".format(\n",
    "                len(df.columns), cn, str(lag), str(forecast)\n",
    "            )\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 46241,
     "status": "ok",
     "timestamp": 1640181143160,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "eHViYf8mYrYE",
    "outputId": "3b1f941b-0e1b-4e61-e179-91f2b31b97be"
   },
   "outputs": [],
   "source": [
    "number_of_features = 3\n",
    "stacked_model(df=fdata, cn='cases', lag=14, forecast=90, plot=True)\n",
    "stacked_model(\n",
    "    df=fdata[feature_selected[: number_of_features + 1]],\n",
    "    cn='cases',\n",
    "    lag=14,\n",
    "    forecast=90,\n",
    "    plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MlGy_smYrYF"
   },
   "source": [
    "### Forecast with Long Short Term Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p06-FeiYrYF"
   },
   "outputs": [],
   "source": [
    "def lstm_model(df, cn, window, forecast, epochs, bs, n_samples=1, seed=True, plot=True):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Train and forecast LSTM. \n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The Dataframe with datetime.date as index with explanatory and response variables  \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        window: int\n",
    "            The preceding window size for time series data construction\n",
    "        forecast: int\n",
    "            The time horizon to forecast\n",
    "        epochs: int\n",
    "            The number of training epochs\n",
    "        bs: int\n",
    "            The number of training batch_size\n",
    "        n_samples: int, default=1\n",
    "            The number of model created for computing confidence interval\n",
    "        plot: boolean, default=True, optional\n",
    "            Visualize plot\n",
    "    ----------\n",
    "    Returns: \n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    data = df.filter([cn]).values\n",
    "    nmin, nmax = data.min(), data.max()\n",
    "    scaled_data = ((df[cn] - nmin) / (nmax - nmin)).values.reshape(-1, 1)\n",
    "    split = round(0.8 * len(data))\n",
    "    train = scaled_data[:split]\n",
    "    trainX, trainy = [], []\n",
    "    for i in range(window, len(train)):\n",
    "        trainX.append(train[i - window : i, 0])\n",
    "        trainy.append(train[i, 0])\n",
    "    trainX, trainy = np.array(trainX), np.array(trainy)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "\n",
    "    tf.random.set_seed(7)  # set seed for reproducibility\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', min_delta=0.003, patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "    lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "    model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        trainy,\n",
    "        epochs=epochs,\n",
    "        batch_size=bs,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    # Testing\n",
    "    test = scaled_data[split - window :]\n",
    "    testX, testy = [], data[split:]\n",
    "    for i in range(window, len(test)):\n",
    "        testX.append(test[i - window : i, 0])\n",
    "    testX = np.array(testX)\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "    prediction = model.predict(testX)\n",
    "    prediction = (nmax - nmin) * prediction + nmin\n",
    "    prediction_series = pd.Series(prediction.flatten(), index=df.index[split:])\n",
    "\n",
    "    # Forecast\n",
    "    results = pd.DataFrame()\n",
    "    for n in range(n_samples):\n",
    "        forecast_list = scaled_data[-window:]\n",
    "        lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "        lstm_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "        lstm_2 = tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "        lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "        model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(\n",
    "            trainX,\n",
    "            trainy,\n",
    "            epochs=epochs,\n",
    "            batch_size=bs,\n",
    "            verbose=0,\n",
    "            callbacks=[early_stopping],\n",
    "        )\n",
    "        for _ in range(forecast):\n",
    "            x = forecast_list[-window:]\n",
    "            x = x.reshape((1, window, 1))\n",
    "            out = model.predict(x)[0][0]\n",
    "            forecast_list = np.append(forecast_list, out)\n",
    "\n",
    "        forecast_list = forecast_list[window - 1 :]\n",
    "        forecast_list = (nmax - nmin) * forecast_list + nmin\n",
    "        results['Forecast{}'.format(n + 1)] = forecast_list.copy()\n",
    "\n",
    "    last_date = df.index.values[-1]\n",
    "    forecast_dates = pd.date_range(last_date, periods=forecast + 1).tolist()\n",
    "    results['Date'] = forecast_dates\n",
    "    results.set_index('Date', inplace=True)\n",
    "    results['Mean'] = results.mean(axis=1)\n",
    "    results['_CI'] = results['Mean'] - results.std(axis=1) / np.sqrt(n_samples) * 1.96\n",
    "    results['+CI'] = results['Mean'] + results.std(axis=1) / np.sqrt(n_samples) * 1.96\n",
    "\n",
    "    # Output\n",
    "    # print('Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | N_samples: {}'\\\n",
    "    #       .format(window, forecast, epochs, bs, n_samples))\n",
    "    # print('RMSE:', np.sqrt(np.mean(testy-prediction)**2))\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(dpi=200)\n",
    "        plt.plot(df[cn][:split])\n",
    "        plt.plot(df[cn][split:])\n",
    "        plt.plot(prediction_series)\n",
    "        plt.plot(results['Mean'])\n",
    "        plt.fill_between(results.index, results['_CI'], results['+CI'], color='#ababab')\n",
    "        plt.legend(\n",
    "            ['Train', 'Test', 'Prediction', 'Forecast', 'CI'], ncol=5, loc='upper left'\n",
    "        )\n",
    "        plt.title(\n",
    "            'Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | N_samples: {} \\nRMSE: {}'.format(\n",
    "                window,\n",
    "                forecast,\n",
    "                epochs,\n",
    "                bs,\n",
    "                n_samples,\n",
    "                np.round(np.sqrt(np.mean(testy - prediction) ** 2)),\n",
    "                4,\n",
    "            ),\n",
    "            color='grey',\n",
    "        )\n",
    "        plt.suptitle(\n",
    "            'Projection: {} Forward Forecast'.format(cn), ha='left', x=0.015, y=0.95\n",
    "        )\n",
    "        plt.figtext(0.85, 0.9, 'LSTM Model', ha='left')\n",
    "        fig.autofmt_xdate()\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.savefig(\n",
    "            \"fig/lstm_model_local_{}_{}_{}_{}_{}\".format(\n",
    "                cn, str(window), str(forecast), str(epochs), str(bs)\n",
    "            )\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 231714,
     "status": "ok",
     "timestamp": 1640181480255,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "8iA706VcYrYF",
    "outputId": "a556cf92-834d-4fde-88ba-c291abe897f7"
   },
   "outputs": [],
   "source": [
    "lstm_model(df=data, cn='cases', window=45, forecast=90, epochs=1, bs=8, plot=True)\n",
    "lstm_model(df=data, cn='cases', window=30, forecast=90, epochs=1, bs=16, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global<a id='global_view'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from cycler import cycler\n",
    "\n",
    "custom_style = {\n",
    "    'figure.autolayout': True,\n",
    "    'figure.titlesize': 20,\n",
    "    'figure.figsize': (10, 5),\n",
    "    'figure.dpi': 100,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.titlesize': 10,\n",
    "    'axes.titlelocation': 'left',\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.grid': True,\n",
    "    'axes.prop_cycle': cycler(\n",
    "        color=['#d73027', '#00518b', '#b1ef89', '#ffd500', '#000000']\n",
    "    ),\n",
    "    'grid.color': '#969696',\n",
    "    'xtick.direction': 'inout',\n",
    "    'ytick.direction': 'inout',\n",
    "    'xtick.minor.visible': True,\n",
    "    'ytick.minor.visible': True,\n",
    "    'ytick.right': True,\n",
    "    'ytick.left': False,\n",
    "    'ytick.labelright': True,\n",
    "    'ytick.labelleft': False,\n",
    "    'xaxis.labellocation': 'right',\n",
    "    'yaxis.labellocation': 'top',\n",
    "    'font.family': 'monospace',\n",
    "    'legend.fontsize': 10,\n",
    "    'legend.loc': 'best',\n",
    "}\n",
    "\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all = pd.read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n",
    "all = all[all['location'] == 'World'][\n",
    "    ['date', 'total_cases', 'new_cases', 'total_deaths', 'new_deaths']\n",
    "].reset_index(drop=True)\n",
    "all.columns = ['date', 'total_cases', 'new_cases', 'total_deceased', 'new_deceased']\n",
    "all = all.iloc[70:]\n",
    "all['date'] = pd.to_datetime(all['date'])\n",
    "all.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    plt.plot(all['total_cases'], label='Total Cases', linewidth=2)\n",
    "    plt.plot(all['new_cases'], label='New Cases', linewidth=2)\n",
    "    plt.plot(all['total_deceased'], label='Total Deceased', linewidth=2)\n",
    "    plt.plot(all['new_deceased'], label='New Deceased', linewidth=2)\n",
    "    ax.set_yscale('log')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Log Scale')\n",
    "    ax.yaxis.set_label_position('right')\n",
    "    plt.xlabel('Date')\n",
    "    plt.suptitle('Global Outlook', ha='left', x=0.015, y=0.95)\n",
    "    plt.savefig(\"fig/global_outlook.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_outbreak_model(df, cn, window, forecast, epochs, bs, seed=True, plot=True):\n",
    "    # Preprocessing\n",
    "    data = df.filter([cn]).values\n",
    "    nmin, nmax = data.min(), data.max()\n",
    "    scaled_data = ((df[cn] - nmin) / (nmax - nmin)).values.reshape(-1, 1)\n",
    "    split = round(0.8 * len(data))\n",
    "    train = scaled_data[:split]\n",
    "    trainX, trainy = [], []\n",
    "    for i in range(window, len(train)):\n",
    "        trainX.append(train[i - window : i, 0])\n",
    "        trainy.append(train[i, 0])\n",
    "    trainX, trainy = np.array(trainX), np.array(trainy)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "\n",
    "    tf.random.set_seed(7)  # set seed for reproducibility\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss', min_delta=0.003, patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "    lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "    model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanSquaredError(name='mse'),\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "        ],\n",
    "    )\n",
    "    model.fit(\n",
    "        trainX,\n",
    "        trainy,\n",
    "        epochs=epochs,\n",
    "        batch_size=bs,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n",
    "    # Testing\n",
    "    test = scaled_data[split - window :]\n",
    "    testX, testy = [], data[split:]\n",
    "    for i in range(window, len(test)):\n",
    "        testX.append(test[i - window : i, 0])\n",
    "    testX = np.array(testX)\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "    prediction = model.predict(testX)\n",
    "    prediction = (nmax - nmin) * prediction + nmin\n",
    "    prediction_series = pd.Series(prediction.flatten(), index=df.index[split:])\n",
    "\n",
    "    # Forecast\n",
    "    forecast_list = scaled_data[-window:]\n",
    "    for _ in range(forecast):\n",
    "        x = forecast_list[-window:]\n",
    "        x = x.reshape((1, window, 1))\n",
    "        out = model.predict(x)[0][0]\n",
    "        forecast_list = np.append(forecast_list, out)\n",
    "\n",
    "    forecast_list = forecast_list[window - 1 :]\n",
    "    forecast_list = (nmax - nmin) * forecast_list + nmin\n",
    "\n",
    "    last_date = df.index.values[-1]\n",
    "    forecast_dates = pd.date_range(last_date, periods=forecast + 1).tolist()\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    results['Forecast'] = forecast_list\n",
    "    results['Date'] = forecast_dates\n",
    "    results.set_index('Date', inplace=True)\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10, 7), dpi=200)\n",
    "        plt.plot(df[cn][:split])\n",
    "        plt.plot(df[cn][split:])\n",
    "        plt.plot(prediction_series)\n",
    "        plt.plot(results['Forecast'])\n",
    "        plt.legend(\n",
    "            ['Train', 'Test', 'Prediction', 'Forecast', 'CI'], ncol=5, loc='upper left'\n",
    "        )\n",
    "        plt.title(\n",
    "            'Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | RMSE: {}'.format(\n",
    "                window,\n",
    "                forecast,\n",
    "                epochs,\n",
    "                bs,\n",
    "                np.round(np.sqrt(np.mean(testy - prediction) ** 2)),\n",
    "                4,\n",
    "            ),\n",
    "            color='grey',\n",
    "        )\n",
    "        plt.suptitle(\n",
    "            'Projection: {} Forward Forecast'.format(cn),\n",
    "            ha='left',\n",
    "            x=0.015,\n",
    "            y=1,\n",
    "            fontsize=18,\n",
    "        )\n",
    "        plt.figtext(0.85, 0.9, 'LSTM Model', ha='left')\n",
    "        fig.autofmt_xdate()\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Cases')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.savefig(\n",
    "            \"fig/lstm_model_global_{}_{}_{}_{}_{}\".format(\n",
    "                cn, str(window), str(forecast), str(epochs), str(bs)\n",
    "            )\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outbreak_model(\n",
    "    df=all, cn='new_cases', window=14, forecast=90, epochs=1, bs=8, plot=True\n",
    ")\n",
    "lstm_outbreak_model(\n",
    "    df=all, cn='new_cases', window=45, forecast=90, epochs=1, bs=8, plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outbreak_model(\n",
    "    df=all, cn='new_deceased', window=14, forecast=90, epochs=1, bs=8, plot=True\n",
    ")\n",
    "lstm_outbreak_model(\n",
    "    df=all, cn='new_deceased', window=45, forecast=90, epochs=1, bs=8, plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies<a id='dependencies'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib==3.5.1\n",
    "# !pip install pandas==1.3.5\n",
    "# !pip install scipy==1.7.1\n",
    "# !pip install lightgbm==3.3.1\n",
    "# !pip install tensorflow==2.7.0\n",
    "# !pip install numpy==1.20.0\n",
    "# !pip install numba==0.50.1\n",
    "# !pip install xgboost==1.5.1\n",
    "# !pip install sklearn==0.23.2\n",
    "# !pip install mlxtend==0.19.0"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
