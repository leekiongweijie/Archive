{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2377bd91",
   "metadata": {},
   "source": [
    "# Playing Around With News Data\n",
    "\n",
    "In this notebook, <br>\n",
    "First part: News from Google News which is an news aggregator (a perfect candidate for bulk news retrieval) is scrape and visualize in network to identify any correlation between trending news among a few countries.\n",
    "\n",
    "Implemented:<br>\n",
    "\n",
    "1. [Google News Scraper](#gnscraper)<br>\n",
    "1. [Using scraper for bulk scrpaing](#bulk_scraping)<br>\n",
    "1. [Plotting in network](#news_network)<br>\n",
    "\n",
    "Second part: Instead of analyst acrossing multiple news media brand, I use twitter to scrape extractly single media to further investigate.\n",
    "\n",
    "1. [Getting Tweets](#tweets)<br>\n",
    "1. [Trending Topic Coverage](#trending_topic)<br>\n",
    "1. [Topic Connectivity And Continuity](#topic_modelling)<br>\n",
    "1. [Sentiment Analysis](#sentiment)<br>\n",
    "1. [WordCloud of Entity](#wcentity)<br>\n",
    "\n",
    "[Dependencies](#dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83873b",
   "metadata": {},
   "source": [
    "## Google News Scraper<a id='gnscraper'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import typing\n",
    "import urllib\n",
    "\n",
    "import feedparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateparser import parse as parse_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, language='en', country='MY') -> None:\n",
    "        \"\"\"\n",
    "        :param str country: two string country code, example: 'MY', 'US'\n",
    "        :param str language: news language\n",
    "        \"\"\"\n",
    "        self.lang = language.lower()\n",
    "        self.country = country.upper()\n",
    "        self.BASE_URL = 'https://news.google.com/rss'\n",
    "\n",
    "    def __news_parser(self, text) -> str:\n",
    "        try:\n",
    "            bs4_html = BeautifulSoup(text, 'html.parser')\n",
    "            lis = bs4_html.find_all('li')\n",
    "            sub_articles = []\n",
    "            for li in lis:\n",
    "                try:\n",
    "                    sub_articles.append(\n",
    "                        {\n",
    "                            'url': li.a['href'],\n",
    "                            'title': li.a.text,\n",
    "                            'publisher': li.font.text,\n",
    "                        }\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "            return sub_articles\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    def __ceid(self) -> dict:\n",
    "        return '?ceid={}:{}&hl={}&gl={}'.format(\n",
    "            self.country, self.lang, self.lang, self.country\n",
    "        )\n",
    "\n",
    "    def __add_sub_articles(self, entries) -> dict:\n",
    "        for i, val in enumerate(entries):\n",
    "            if 'summary' in entries[i].keys():\n",
    "                entries[i]['sub_articles'] = self.__news_parser(entries[i]['summary'])\n",
    "            else:\n",
    "                entries[i]['sub_articles'] = None\n",
    "\n",
    "        return entries\n",
    "\n",
    "    def __parse_feed(self, feed_url, proxies=None) -> dict:\n",
    "        if proxies:\n",
    "            r = requests.get(feed_url, proxies=proxies)\n",
    "        else:\n",
    "            r = requests.get(feed_url)\n",
    "\n",
    "        r = requests.get(feed_url)\n",
    "\n",
    "        if \"https://news.google.com/rss/unsupported\" in r.url:\n",
    "            raise Exception('This feed is not available')\n",
    "\n",
    "        d = feedparser.parse(r.text)\n",
    "\n",
    "        if not proxies and len(d['entries']) == 0:\n",
    "            d = feedparser.parse(feed_url)\n",
    "\n",
    "        return dict((k, d[k]) for k in ('feed', 'entries'))\n",
    "\n",
    "    def __search_helper(self, query):\n",
    "        return urllib.parse.quote_plus(query)\n",
    "\n",
    "    def __from_to_helper(self, validate=None) -> typing.Optional[str]:\n",
    "        try:\n",
    "            validate = parse_date(validate).strftime('%Y-%m-%d')\n",
    "            return str(validate)\n",
    "        except:\n",
    "            raise Exception('Could not parse your date')\n",
    "\n",
    "    def __extract_summary(self, text: str) -> list:\n",
    "        result = list()\n",
    "        length = len(text.split('target='_blank'>'))\n",
    "\n",
    "        if length > 2:\n",
    "            for i in text.split('target='_blank'>')[:-1]:\n",
    "                if '</a>' not in i:\n",
    "                    continue\n",
    "                else:\n",
    "                    text = i.split('</a>')[0]\n",
    "                    result.append(text)\n",
    "            result = '. '.join(result)\n",
    "        else:\n",
    "            if '</a>' in text:\n",
    "                text = text.split('target='_blank'>')[1]\n",
    "                text = text.split('</a>')[0]\n",
    "                result.append(text)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __clean_news(self, r, n: int, show: bool) -> dict:\n",
    "        r = copy.copy(r)\n",
    "        required, present = 0, len(r.get('entries'))\n",
    "\n",
    "        if n < present:\n",
    "            required = copy.copy(n)\n",
    "        if n > present:\n",
    "            required = copy.copy(present)\n",
    "\n",
    "        titles, publishers, published_times, summaries, links = [], [], [], [], []\n",
    "\n",
    "        i = 0\n",
    "        for i in range(required):\n",
    "            if r.get('entries')[i].get('title').count('-') > 1:\n",
    "                title = r.get('entries')[i].get('title').rsplit('-', 1)[0]\n",
    "                publisher = r.get('entries')[i].get('title').rsplit('-', 1)[-1].strip()\n",
    "            else:\n",
    "                title = r.get('entries')[i].get('title').split('-')[0]\n",
    "                publisher = r.get('entries')[i].get('title').split('-')[-1].strip()\n",
    "            published_time = r.get('entries')[i].get('published')\n",
    "            raw_summary = r.get('entries')[i].get('summary')\n",
    "            summary = self.__extract_summary(raw_summary)\n",
    "            link = r.get('entries')[i].get('link')\n",
    "\n",
    "            titles.append(title)\n",
    "            publishers.append(publisher)\n",
    "            published_times.append(published_time)\n",
    "            summaries.append(summary)\n",
    "            links.append(link)\n",
    "\n",
    "            if show:\n",
    "                print('Title: ', title)\n",
    "                print('Publisher: ', publisher)\n",
    "                print('Published Time: ', published_time)\n",
    "                print('Summary: ', summary)\n",
    "                print('Link: ', link)\n",
    "                print('\\n')\n",
    "\n",
    "        return {\n",
    "            'titles': titles,\n",
    "            'publishers': publishers,\n",
    "            'published_times': published_times,\n",
    "            'summaries': summaries,\n",
    "            'links': links,\n",
    "        }\n",
    "\n",
    "    def get_news(self, nums: int, show: bool, proxies=None) -> dict:\n",
    "        \"\"\"\n",
    "        :param int nums: number of news to retrieve\n",
    "        :param bool show: print searched results\n",
    "        :return dict d: dictionary of curated results\n",
    "        \"\"\"\n",
    "        d = self.__parse_feed(self.BASE_URL + self.__ceid(), proxies=proxies)\n",
    "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
    "        d = self.__clean_news(d, nums, show)\n",
    "\n",
    "        return d\n",
    "\n",
    "    def get_news_by_topics(\n",
    "        self, topic: str, nums: int, show: bool, proxies=None\n",
    "    ) -> typing.Optional[dict]:\n",
    "        \"\"\"\n",
    "        :param str topic: news topic to query\n",
    "        :param int nums: number of news to retrieve\n",
    "        :param bool show: print searched results\n",
    "        :return dict d: dictionary of curated results\n",
    "        \"\"\"\n",
    "        if topic.upper() in [\n",
    "            'WORLD',\n",
    "            'NATION',\n",
    "            'BUSINESS',\n",
    "            'TECHNOLOGY',\n",
    "            'ENTERTAINMENT',\n",
    "            'SCIENCE',\n",
    "            'SPORTS',\n",
    "            'HEALTH',\n",
    "        ]:\n",
    "            d = self.__parse_feed(\n",
    "                self.BASE_URL\n",
    "                + '/headlines/section/topic/{}'.format(topic.upper())\n",
    "                + self.__ceid(),\n",
    "                proxies=proxies,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            d = self.__parse_feed(\n",
    "                self.BASE_URL + '/topics/{}'.format(topic) + self.__ceid(),\n",
    "                proxies=proxies,\n",
    "            )\n",
    "\n",
    "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
    "        if len(d['entries']) > 0:\n",
    "            d = self.__clean_news(d, nums, show)\n",
    "            return d\n",
    "        else:\n",
    "            raise Exception('unsupported topic')\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        nums: int,\n",
    "        show: bool,\n",
    "        helper=True,\n",
    "        when=None,\n",
    "        from_=None,\n",
    "        to_=None,\n",
    "        proxies=None,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        :param str query: news title to query\n",
    "        :param int nums: number of news to retrieve\n",
    "        :param bool show: print searched results\n",
    "        :param str when: results in an article published in last _, example: '30m', '1h', '7d'\n",
    "        :return dict d: dictionary of curated results\n",
    "        \"\"\"\n",
    "        if when:\n",
    "            query += ' when:' + when\n",
    "\n",
    "        if from_ and not when:\n",
    "            from_ = self.__from_to_helper(validate=from_)\n",
    "            query += ' after:' + from_\n",
    "\n",
    "        if to_ and not when:\n",
    "            to_ = self.__from_to_helper(validate=to_)\n",
    "            query += ' before:' + to_\n",
    "\n",
    "        if helper == True:\n",
    "            query = self.__search_helper(query)\n",
    "\n",
    "        search_ceid = self.__ceid()\n",
    "        search_ceid = search_ceid.replace('?', '&')\n",
    "\n",
    "        d = self.__parse_feed(\n",
    "            self.BASE_URL + '/search?q={}'.format(query) + search_ceid, proxies=proxies\n",
    "        )\n",
    "        d['entries'] = self.__add_sub_articles(d['entries'])\n",
    "        d = self.__clean_news(d, nums, show)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample\n",
    "scraper = Scraper()\n",
    "news = scraper.get_news(nums=1, show=True)\n",
    "news_topics = scraper.get_news_by_topics(topic='science', nums=1, show=True)\n",
    "news_searched = scraper.search(query='5G', nums=1, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc4a83",
   "metadata": {},
   "source": [
    "## Bulk Scraping Google News<a id='bulk_scraping'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43bdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_detect_translate(\n",
    "    text: str, source_language: str = None, trg_language: str = None\n",
    ") -> str:\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    if source_language == None:\n",
    "        source_language = detect(text)\n",
    "        lang_dict = {\n",
    "            'en': 'en',\n",
    "            'zh-cn': 'zh-CN',\n",
    "            'zh-tw': 'zh-TW',\n",
    "            'de': 'de',\n",
    "            'fr': 'fr',\n",
    "            'ko': 'ko',\n",
    "            'ja': 'ja',\n",
    "            'id': 'id',\n",
    "        }\n",
    "        source_language = lang_dict.get(source_language)\n",
    "\n",
    "        if source_language == None:\n",
    "            return None\n",
    "\n",
    "    if source_language == trg_language:\n",
    "        return text\n",
    "    else:\n",
    "        # print('Translating news')\n",
    "        translator = GoogleTranslator(source=source_language, target=trg_language)\n",
    "        translated_text = translator.translate(text)\n",
    "        # GoogleTranslator.get_supported_languages(as_dict=True)\n",
    "        translated_text = translator.translate(text)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "\n",
    "def detect_cjk(text: str) -> bool:\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "    if re.search('[\\u4e00-\\u9FFF]', text):\n",
    "        return False\n",
    "    if re.search('[\\uac00-\\ud7a3]', text):\n",
    "        return False\n",
    "    if re.search('[\\u3040-\\u30ff]', text):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocessing_news(country_list: list) -> pd.DataFrame:\n",
    "    news_dict = dict()\n",
    "\n",
    "    for c in tqdm.tqdm(country_list):\n",
    "        extracted_news = list()\n",
    "        country_lang_dict = {\n",
    "            'MY': 'en',\n",
    "            'US': 'en',\n",
    "            'GB': 'en',\n",
    "            'IN': 'en',\n",
    "            'CN': 'CN',\n",
    "            'DE': 'de',\n",
    "            'FR': 'fr',\n",
    "            'TW': 'TW',\n",
    "            'HK': 'TW',\n",
    "            'AU': 'en',\n",
    "            'KR': 'ko',\n",
    "            'JP': 'ja',\n",
    "            'CA': 'en',\n",
    "            'SG': 'en',\n",
    "            'ID': 'en',\n",
    "            'NZ': 'en',\n",
    "            'IE': 'en',\n",
    "            'IL': 'en',\n",
    "            'PK': 'en',\n",
    "            'ZA': 'en',\n",
    "            'CH': 'de',\n",
    "            'IT': 'it',\n",
    "        }\n",
    "\n",
    "        language_use = country_lang_dict.get(c)\n",
    "        scraper = Scraper(language=language_use, country=c)\n",
    "\n",
    "        try:\n",
    "            news = scraper.get_news_by_topics(topic='world', nums=50, show=False)\n",
    "        except:\n",
    "            print('Unable to get news for {}'.format(c))\n",
    "            pass\n",
    "\n",
    "        lang = 'zh-TW' if c == 'HK' or c == 'TW' else ('zh-CN' if c == 'CN' else None)\n",
    "        if lang == None:\n",
    "            lang = country_lang_dict.get(c)\n",
    "\n",
    "        for i in range(len(news.get('titles'))):\n",
    "            cleaned_text = clean_detect_translate(\n",
    "                news.get('titles')[i], source_language=lang, trg_language='en'\n",
    "            )\n",
    "            if detect_cjk(text=cleaned_text):\n",
    "                extracted_news.append(cleaned_text)\n",
    "\n",
    "        # print('Successfully scrape {} news'.format(c)) if len(extracted_news) > \\\n",
    "        # 0 else print('Failed to scrape {} news'.format(c))\n",
    "        news_dict[c] = extracted_news\n",
    "        # print(c)\n",
    "        # print(extracted_news)\n",
    "        # print('\\n')\n",
    "\n",
    "    compiled_news = pd.DataFrame()\n",
    "    for c in country_list:\n",
    "        compiled_news = pd.concat(\n",
    "            [compiled_news, pd.DataFrame({c: news_dict.get(c)})], axis=1\n",
    "        )\n",
    "\n",
    "    return compiled_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4e0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = [\n",
    "    'MY',\n",
    "    'US',\n",
    "    'GB',\n",
    "    'IN',\n",
    "    'CN',\n",
    "    'DE',\n",
    "    'FR',\n",
    "    'TW',\n",
    "    'HK',\n",
    "    'AU',\n",
    "    'KR',\n",
    "    'JP',\n",
    "    'CA',\n",
    "    'SG',\n",
    "    'ID',\n",
    "    'NZ',\n",
    "    'IE',\n",
    "    'IL',\n",
    "    'PK',\n",
    "    'ZA',\n",
    "    'CH',\n",
    "    'IT',\n",
    "]\n",
    "\n",
    "news_data = preprocessing_news(country_list=COUNTRIES)\n",
    "news_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cfffc",
   "metadata": {},
   "source": [
    "## News meets Network<a id='news_network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn import feature_extraction, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea59160",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37651728",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Levenshtein Distance\n",
    "def levenshtein_dis(seq1: str, seq2: str) -> int:\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix[x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix[0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x - 1] == seq2[y - 1]:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x - 1, y] + 1, matrix[x - 1, y - 1], matrix[x, y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix[x, y] = min(\n",
    "                    matrix[x - 1, y] + 1, matrix[x - 1, y - 1] + 1, matrix[x, y - 1] + 1\n",
    "                )\n",
    "\n",
    "    # print (matrix)\n",
    "    return matrix[size_x - 1, size_y - 1]\n",
    "\n",
    "\n",
    "## Cosine Similarity and TFIDF\n",
    "def cosimilarity_tfidf(text1, text2) -> int:\n",
    "\n",
    "    text_list = list([text1, text2])\n",
    "\n",
    "    vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "    textX = vectorizer.fit_transform(text_list)\n",
    "\n",
    "    return metrics.pairwise.cosine_similarity(textX[0], textX[1]).flatten()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d1ca1",
   "metadata": {},
   "source": [
    "### News Similarity from Single Country as Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e44f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def single_news_network(\n",
    "    country: str, search_keywords: str, nums: int, when: str, formula: int\n",
    ") -> None:\n",
    "    scraper = Scraper(country=country)\n",
    "    searched_news = scraper.search(\n",
    "        query=search_keywords, nums=nums, show=False, when=when\n",
    "    )\n",
    "    titles_list = searched_news.get('titles')\n",
    "\n",
    "    sources_distances = list()\n",
    "    sources_distances_df = pd.DataFrame(index=titles_list)\n",
    "\n",
    "    if formula == 1:\n",
    "        for i in range(len(titles_list)):\n",
    "            for j in range(len(titles_list)):\n",
    "                sources_distances.append(\n",
    "                    cosimilarity_tfidf(titles_list[i], titles_list[j])\n",
    "                )\n",
    "    elif formula == 2:\n",
    "        for i in range(len(titles_list)):\n",
    "            for j in range(len(titles_list)):\n",
    "                sources_distances.append(\n",
    "                    levenshtein_dis(titles_list[i], titles_list[j])\n",
    "                )\n",
    "\n",
    "    chunks = [\n",
    "        sources_distances[i : i + len(titles_list)]\n",
    "        for i in range(0, len(sources_distances), len(titles_list))\n",
    "    ]\n",
    "\n",
    "    for i, t in enumerate(titles_list):\n",
    "        sources_distances_df[t] = chunks[i]\n",
    "\n",
    "    stacked_df = sources_distances_df.stack().reset_index()\n",
    "    stacked_df.columns = ['Source_1', 'Source_2', 'Distances']\n",
    "\n",
    "    filtered_stacked_df = stacked_df.loc[\n",
    "        (stacked_df['Distances'] > 0)\n",
    "        & (stacked_df['Source_1'] != stacked_df['Source_2'])\n",
    "    ]\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        filtered_stacked_df, source='Source_1', target='Source_2'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(100, 50), dpi=200)\n",
    "    plt.title(\n",
    "        'News Similarity under the same keywords: {}'.format(search_keywords),\n",
    "        fontsize=75,\n",
    "    )\n",
    "    nx.draw(\n",
    "        G,\n",
    "        with_labels=True,\n",
    "        node_color='orange',\n",
    "        node_size=400,\n",
    "        edge_color='grey',\n",
    "        style='dashed',\n",
    "        linewidths=1,\n",
    "        font_size=30,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "single_news_network(\n",
    "    country='MY', search_keywords='blockchain', nums=20, when='12h', formula=2\n",
    ")  # 1 for lev_dis, 2 for cosine_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a922d3",
   "metadata": {},
   "source": [
    "### News Similarity Multiple Country as Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def multi_news_networks(\n",
    "    country: list, search_keywords: str, nums: int, when: str\n",
    ") -> None:\n",
    "\n",
    "    scraper_list, titles_list_of_list, titles_list = ([] for i in range(3))\n",
    "\n",
    "    for c in country:\n",
    "        scraper_list.append(Scraper(country=c))\n",
    "\n",
    "    for i in range(len(country)):\n",
    "        searched_news = scraper_list[i].search(\n",
    "            query=search_keywords, nums=nums, show=False, when=when\n",
    "        )\n",
    "        titles = searched_news.get('titles')\n",
    "        titles_list_of_list.append(titles)\n",
    "\n",
    "    titles_df = pd.DataFrame(data=titles_list_of_list).T\n",
    "    titles_df.columns = country.copy()\n",
    "    titles_df = pd.melt(titles_df, value_vars=country)\n",
    "    titles_df.rename(columns={'variable': 'country', 'value': 'title'}, inplace=True)\n",
    "\n",
    "    titles_list = [\n",
    "        t for sublist in titles_list_of_list for t in sublist\n",
    "    ]  # list of list to list\n",
    "    titles_list = list(set(titles_list))  # remove duplicates title\n",
    "\n",
    "    node_characteristic = pd.DataFrame(\n",
    "        {'ID': country + titles_list, 'type': country + ['t'] * len(titles_list)}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(50, 50), dpi=150)\n",
    "    plt.title('Multicountry News Similarity: The significant', fontsize=50)\n",
    "\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        titles_df, source='country', target='title', create_using=nx.Graph()\n",
    "    )\n",
    "\n",
    "    node_characteristic = node_characteristic.set_index('ID')\n",
    "    node_characteristic = node_characteristic.reindex(G.nodes())\n",
    "    node_characteristic['type'] = pd.Categorical(node_characteristic['type'])\n",
    "\n",
    "    cmap = mpl.colors.ListedColormap(\n",
    "        ['yellow', 'C0', 'green', 'red', 'darkorange', 'thistle']\n",
    "    )\n",
    "\n",
    "    node_sizes = [4000 if entry != 't' else 300 for entry in node_characteristic.type]\n",
    "\n",
    "    nx.draw(\n",
    "        G,\n",
    "        with_labels=True,\n",
    "        node_size=node_sizes,\n",
    "        node_color=node_characteristic['type'].cat.codes,\n",
    "        cmap=cmap,\n",
    "        edge_color='grey',\n",
    "        style='dashed',\n",
    "        linewidths=1,\n",
    "        font_size=20,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "multi_news_networks(\n",
    "    country=['MY', 'US', 'GB', 'SG', 'IN'],\n",
    "    search_keywords='blockchain',\n",
    "    nums=20,\n",
    "    when='12h',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852ca4c",
   "metadata": {},
   "source": [
    "## Getting Tweets<a id='tweets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee234e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_timeline(username: str, count: int, to_date) -> pd.DataFrame:\n",
    "\n",
    "    tweets = list()\n",
    "    tweets_df = list()\n",
    "\n",
    "    result = api.user_timeline(id=username, count=count, tweet_mode='extended')\n",
    "    for t in result:\n",
    "        tweets.append(t)\n",
    "\n",
    "    try:\n",
    "        while result[-1].created_at > to_date:\n",
    "            # time.sleep(2)\n",
    "            print('Last tweet @', result[-1].created_at, 'querying more')\n",
    "            result = api.user_timeline(\n",
    "                id=username,\n",
    "                count=count,\n",
    "                max_id=result[-1].id - 1,\n",
    "                tweet_mode='extended',\n",
    "            )\n",
    "            for t in result:\n",
    "                tweets.append(t)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for t in tweets:\n",
    "        t_created_at = t._json['created_at']\n",
    "        t_tweet_id = t._json['id']\n",
    "        t_text = t._json['full_text']\n",
    "        tweets_df.append([t_created_at, t_tweet_id, t_text])\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_df, columns=['created_at', 'tweet_id', 'text'])\n",
    "    return tweets, tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = datetime.datetime.now() - datetime.timedelta(1)\n",
    "to_date = datetime.datetime(yesterday.year, yesterday.month, yesterday.day)\n",
    "USERNAME = 'TheEconomist'\n",
    "count = 10\n",
    "\n",
    "tweets, tweets_df = query_timeline(USERNAME, count, to_date)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b0506",
   "metadata": {},
   "source": [
    "## Helper Functios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aefa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_process(word_token: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = list()\n",
    "    for word, tag in pos_tag(word_token):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text: str):\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('(@[A-Za-z0-9]+)', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    tokenizer = TweetTokenizer()\n",
    "    word_token = tokenizer.tokenize(text)\n",
    "\n",
    "    new_words = ['rt', 'RT']\n",
    "    stop_words = set(corpus.stopwords.words('english') + new_words)\n",
    "    word_token = [w for w in word_token if w not in stop_words]\n",
    "\n",
    "    # word_token = [w for w in word_token if len(w) >= 3]\n",
    "    # stemmer = PorterStemmer()\n",
    "    # word_token = [stemmer.stem(word) for word in word_token]\n",
    "    # word_token = lemmatize_process(word_token)\n",
    "\n",
    "    return ' '.join(word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cccb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text2(text: str) -> str:\n",
    "    text = re.sub('(@[A-Za-z0-9]+)', '', text)\n",
    "    text = re.sub(r'http\\S+', r'', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = text.replace('RT', '')\n",
    "    # text = text.lstrip()\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb005b99",
   "metadata": {},
   "source": [
    "## Trending Topic Coverage<a id='trending_topic'></a>\n",
    "\n",
    "Extract keywords from tweets [^1],[^2],[^3],[^4],[^5], using Google Trends [^6] as benchmark (everyone Google) to evaluate the hotness(relevancy) of trending topic. Matcher [^7] with predefined word pattern is use to filter extracted keyword. To penalty lengthy tweets, the score constitute two compartment, keyword score and length score whereas 90|10 is assigned with hundred percent as max. Optimal tweet length in this project define at 85 characters (71-100/2). Average interest is computed with each keywords's interest in a single tweets.\n",
    "\n",
    "References: <br>\n",
    "Keywords extration -\n",
    "[1](https://towardsdatascience.com/keyword-extraction-a-benchmark-of-7-algorithms-in-python-8a905326d93f)\n",
    "[2](https://www.kaggle.com/akhatova/extract-keywords)<br>\n",
    "Google trends -\n",
    "[1](https://github.com/GeneralMills/pytrends)\n",
    "[2](https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories)<br>\n",
    "Optimal length -\n",
    "[1](https://buffer.com/library/optimal-length-social-media/#:~:text=The%20optimal%20length%20of%20a%20tweet%20%E2%80%94%2071%20to%20100%20characters)\n",
    "[2](https://influencermarketinghub.com/best-length-for-social-media-posts/)<br>\n",
    "\n",
    "As of 2022-03-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import typing\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import spacy\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "from nltk import corpus\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from pytrends.request import TrendReq as UTrendReq\n",
    "from rake_nltk import Rake\n",
    "from scipy import stats\n",
    "from spacy.matcher import Matcher\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "bert = KeyBERT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5281ef1",
   "metadata": {},
   "source": [
    "### Define Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0eb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extractor(text: str) -> typing.Optional[list]:\n",
    "    kw = nlp(text)\n",
    "    kw = list(kw.ents)\n",
    "    return [str(w) for w in kw]\n",
    "\n",
    "\n",
    "def rake_extractor(text: str) -> typing.Optional[list]:\n",
    "    r = Rake(max_length=2, include_repeated_phrases=False)\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return r.get_ranked_phrases()[:5]\n",
    "\n",
    "\n",
    "def gensim_extractor(text: str) -> typing.Optional[list]:\n",
    "    kw = gensim.summarization.keywords(text, split=True, lemmatize=True, deacc=True)\n",
    "    return kw\n",
    "\n",
    "\n",
    "def yake_extractor(text: str) -> typing.Optional[list]:\n",
    "    kw = yake.KeywordExtractor(lan='en', n=2, dedupLim=0.9).extract_keywords(text)\n",
    "    return [w[0] for w in kw][:5]\n",
    "\n",
    "\n",
    "def bert_extractor(text: str) -> typing.Optional[list]:\n",
    "    kw = bert.extract_keywords(\n",
    "        text, keyphrase_ngram_range=(1, 2), use_mmr=True, diversity=0.7\n",
    "    )\n",
    "    return [w[0] for w in kw][:5]\n",
    "\n",
    "\n",
    "def extract_aggregator(raw_text: str, preprocessed_text: str) -> typing.Optional[list]:\n",
    "    # extractor = [spacy_extractor, rake_extractor, gensim_extractor, yake_extractor, bert_extractor]\n",
    "    # suitable_text = [preprocessed_text, raw_text, preprocessed_text, preprocessed_text, preprocessed_text]\n",
    "    suitable_text = [raw_text, preprocessed_text, preprocessed_text]\n",
    "    extractor = [rake_extractor, yake_extractor, bert_extractor]\n",
    "    # for e in extractor:\n",
    "    #     print('{} with raw text: {}'.format(str(e.__name__), e(raw_text)))\n",
    "    #     print('{} with raw text: {}'.format(str(e.__name__), e(preprocessed_text)))\n",
    "    #     print('\\n')\n",
    "\n",
    "    keywords_list_of_list = [e(t) for e, t in zip(extractor, suitable_text)]\n",
    "    all_keywords = [w for kw_list in keywords_list_of_list for w in kw_list]\n",
    "    return all_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db524c4",
   "metadata": {},
   "source": [
    "### Pattern Matching for Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(keyword: str) -> bool:\n",
    "    patterns = [\n",
    "        [{'POS': 'PROPN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'PROPN'}, {'POS': 'PROPN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "        [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "    ]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add('pos-matcher', patterns)\n",
    "    doc = nlp(keyword)\n",
    "    matches = matcher(doc, as_spans=True)\n",
    "    return True if len(matches) > 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ab4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matcher(keywords_list: list) -> typing.Optional[list]:\n",
    "    qualified_keywords_list = list()\n",
    "    for kw in keywords_list:\n",
    "        if match(kw):\n",
    "            qualified_keywords_list.append(kw)\n",
    "            # qualified_keywords_list.extend(kw)\n",
    "\n",
    "    qualified_keywords_list = list(dict.fromkeys(qualified_keywords_list))\n",
    "\n",
    "    return qualified_keywords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae125a9",
   "metadata": {},
   "source": [
    "### Interest Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress printing from a function with @supress_stdout decorator\n",
    "def supress_stdout(func):\n",
    "    def wrapper(*a, **ka):\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            with contextlib.redirect_stdout(devnull):\n",
    "                return func(*a, **ka)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b54acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50571317/pytrends-the-request-failed-google-returned-a-response-with-code-429\n",
    "# https://github.com/GeneralMills/pytrends/issues/369\n",
    "\n",
    "headers = {\n",
    "    'authority': 'trends.google.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    # Requests sorts cookies= alphabetically\n",
    "    # 'cookie': '__utma=10102256.912980743.1646273337.1648110105.1648954678.12; __utmz=10102256.1648954678.12.10.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); __utmc=10102256; __utmt=1; __utmb=10102256.4.9.1648954699511; SEARCH_SAMESITE=CgQIk5QB; HSID=Adt-kzWvriIqL_C-6; SSID=AAS2WPOTsuEs7Z7U0; APISID=AzfErRa5847V_lok/AZu6H-u0FGHzfAYoj; SAPISID=jmwWfOBRs4eWnGwN/AzGKH00UU6t-X1dPK; __Secure-1PAPISID=jmwWfOBRs4eWnGwN/AzGKH00UU6t-X1dPK; __Secure-3PAPISID=jmwWfOBRs4eWnGwN/AzGKH00UU6t-X1dPK; SID=IQjsXvn7hMEiJ6yMEs52o8xXnk6Wj2EV_r9DTXfGpg-sgozU2_DaNvoyKL6WAD0J0XMnWQ.; __Secure-1PSID=IQjsXvn7hMEiJ6yMEs52o8xXnk6Wj2EV_r9DTXfGpg-sgozUqNw5STWS4KVh9tq64kzvIg.; __Secure-3PSID=IQjsXvn7hMEiJ6yMEs52o8xXnk6Wj2EV_r9DTXfGpg-sgozUfWbHxyAmcW5QeURO8kZrww.; AEC=AVQQ_LC6H13rsTBRK_BWKQHbwwRFokgnDNYiSCCHysyzthb8E-cm-Ih_Cw; 1P_JAR=2022-04-03-02; NID=511=uHvmScTmiFPM7AePqt_iTPyTu1iz59YYIo0kv7hoSJMd5ZNeDcQb4I8PxN8Av53wnGKHfP1oQ6fErGdnpFN8-BGFvDVABxeiu5eMSYxQb04Ckr4Y_QbOBoRLU3M7xtPMWjiP1MVWlLYF-0ZtMh9vjHNh0zMsXbK6wZ-pJJpix5S0-6mQd0w8JeWsrLmGhpJBhPaMk9wqHFCGCBpudXi6Vkhgm9HeUXdeSqKvVPzeOuZ1wPbTHq6xS-Wmr_T1hJ53nPHAXKjxiLfznWE9mCS1EZAWLtxB84FRTmYllmZCbHlqFWPa71fBjSa-xMnmDerwm2VlTqTAmY-AaiUmKSm9nw7BvO8wi8RjgnRlstL6kkbMSXOixUZ5fMvBdFQ8ssEcne7uWFRW-GbU2jTOjkw_pdr5T7m5XHnp_8fxNFKJ6_b1vEUBcXB3GTaFnXnCHwi2jpztJHkg_u_w0GHf2zjKQDUG; SIDCC=AJi4QfEyIfFUwtE8qTLeRja8wT2nUNDIAgxgShPmV5CK9rZFBjcKZQu71NJuDswrsmzU0XGtpLg; __Secure-3PSIDCC=AJi4QfFN8DbWAYPKXrK4_xgqteQtyQM0pwXa2yDQ80L9nCj7FE1baLmsJTD_TkmySdnS4xx3Vizc',\n",
    "    'referer': 'https://trends.google.com/trends/?geo=MY',\n",
    "    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"100\", \"Google Chrome\";v=\"100\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36',\n",
    "    'x-client-data': 'CLC1yQEIiLbJAQijtskBCMG2yQEIqZ3KAQiM0soBCISDywEIlqHLAQjr8ssBCJ75ywEI54TMAQiljswBCJqPzAEImaHMAQjPoswBCIGkzAEIsqTMAQ==',\n",
    "}\n",
    "\n",
    "\n",
    "GET_METHOD = 'GET'\n",
    "\n",
    "\n",
    "class TrendReq(UTrendReq):\n",
    "    def _get_data(self, url, method=GET_METHOD, trim_chars=0, **kwargs):\n",
    "        return super()._get_data(\n",
    "            url, method=GET_METHOD, trim_chars=trim_chars, headers=headers, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interest_retriever(keywords_list: list, tweet_date) -> int:\n",
    "    '''\n",
    "    Input with a list of keywords from a single tweet and tweet date(d), get interest for each keyword with d-3 days.\n",
    "    ...\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    keywords_list: list of string\n",
    "        A list containing keywords as string from a single tweet\n",
    "    tweet_date: date\n",
    "        The correspond tweet date\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    interest_score: int\n",
    "        The correspond interest score of the input tweet\n",
    "\n",
    "    '''\n",
    "    from pytrends.exceptions import ResponseError\n",
    "\n",
    "    if 'pytrends' not in locals():\n",
    "        pytrend = TrendReq()\n",
    "\n",
    "    date = '{} {}'.format(tweet_date[:10], tweet_date[:10])\n",
    "    max_n = 5\n",
    "    iteration = len(keywords_list) / max_n\n",
    "    results_interest = list()\n",
    "\n",
    "    for _ in range(math.ceil(iteration)):\n",
    "        try:\n",
    "            pytrend.build_payload(kw_list=keywords_list[:max_n], cat=16, timeframe=date)\n",
    "            if pytrend.interest_over_time().shape[0] != 0:\n",
    "                results = pytrend.interest_over_time().iloc[0, :-1].values\n",
    "                results_interest.append(results.tolist())\n",
    "                del keywords_list[:max_n]\n",
    "                # display(pytrend.interest_over_time())\n",
    "        except ResponseError:\n",
    "            pass\n",
    "        except requests.exceptions.Timeout:\n",
    "            pass\n",
    "\n",
    "    results_interest = [item for sublist in results_interest for item in sublist]\n",
    "    # print('results_interest: ', results_interest)\n",
    "\n",
    "    if len(results_interest) != 0:\n",
    "        interest_score = sum(results_interest) / len(results_interest)\n",
    "        # print(interest_score)\n",
    "        return round(interest_score, 4)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7e8f7",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a576c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_topic_coverage(df, text_col: str, date_col: str) -> dict:\n",
    "    date_list = pd.to_datetime(df[date_col]).tolist()\n",
    "    s_docs = df[text_col].apply(lambda x: clean_the_text2(x)).tolist()\n",
    "    f_docs = df[text_col].apply(lambda x: clean_the_text(x)).tolist()\n",
    "\n",
    "    scores = list()\n",
    "    iteration = len(df)\n",
    "    score_dict = dict()\n",
    "\n",
    "    for i in tqdm(range(iteration)):\n",
    "        raw_text, preprocessed_text = s_docs[i], f_docs[i]\n",
    "        all_keywords = extract_aggregator(raw_text, preprocessed_text)\n",
    "        qualified_keywords_list = pattern_matcher(all_keywords)\n",
    "        tweet_date = date_list[i] - datetime.timedelta(3)\n",
    "        text_length = len(df[text_col][i])\n",
    "\n",
    "        if len(qualified_keywords_list) != 0:\n",
    "            keyword_score = interest_retriever(qualified_keywords_list, str(tweet_date))\n",
    "            length_score = np.where(\n",
    "                text_length > 170,\n",
    "                0,\n",
    "                np.where(\n",
    "                    text_length <= 85, text_length / 85, abs(text_length / 85 - 2)\n",
    "                ),\n",
    "            )\n",
    "            score = (keyword_score * 0.8) + (length_score / 0.2)\n",
    "            scores.append(score)\n",
    "\n",
    "        print('Text: ', df[text_col][i])\n",
    "        print('Total Score: ', score)\n",
    "        print('\\n')\n",
    "\n",
    "        score_dict[df[text_col][i]] = score\n",
    "\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13913b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttc_results = trending_topic_coverage(\n",
    "    df=tweets_df, text_col='text', date_col='created_at'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965619a",
   "metadata": {},
   "source": [
    "### PLot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15174169",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc_results = pd.DataFrame(ttc_results, index=['score']).T.reset_index()\n",
    "ttc_results.columns = ['text', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data: typing.Optional[str] = pd.Series):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    n = len(data)\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, n + 1) / n\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), dpi=200)\n",
    "ax[0].axvline(x=ttc_results['score'].median(), label='Median', color='red')\n",
    "ax[0].axvline(\n",
    "    x=ttc_results['score'].quantile(0.75), label='75th quantile', color='orange'\n",
    ")\n",
    "ttc_results['score'].plot(kind='hist', ax=ax[0], title='Score Distributions')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Score')\n",
    "# In an ECDF, the x-axis is the range of possible values for the data & for any given x-value, the correspond y-value is the\n",
    "# proportion of data points less than or equal to that x-value.\n",
    "ecdf_x, ecdf_y = ecdf(ttc_results['score'])\n",
    "ax[1].plot(ecdf_x, ecdf_y, marker='.', linestyle='none')\n",
    "ax[1].set_title('With ECDF')\n",
    "ax[1].set_xlabel('Score')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7b184",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic Modelling for Topic Connectivity And Continuity<a id='topic_modelling'></a>\n",
    "\n",
    "Topic modelling clustering implemented to identify or discover possible abstract topic among the tweets. With pretrained SentenceTransformer [^8] to extract various embeddings based on the context of tweets, Uniform Manifold Approximation and Projection(umap) [^9] to lower the dimension before clustering with HDBSCAN [^10] and later using c-TF-IDF for topic creation.\n",
    "\n",
    "References: <br>\n",
    "[1](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)\n",
    "[2](https://towardsdatascience.com/dynamic-topic-modeling-with-bertopic-e5857e29f872)<br>\n",
    "\n",
    "As of 2022-03-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "import umap\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231559dd",
   "metadata": {},
   "source": [
    "### C-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa90d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents: np.array, m: int, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(\n",
    "        documents\n",
    "    )\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8e859",
   "metadata": {},
   "source": [
    "### Top Words for Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic_topic, n=20) -> dict:\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic_topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {\n",
    "        label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1]\n",
    "        for i, label in enumerate(labels)\n",
    "    }\n",
    "    return top_n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20314546",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelling(df, text_col: str) -> pd.DataFrame:\n",
    "    raw_text = df[text_col].values\n",
    "    docs = df[text_col].apply(lambda x: clean_the_text(x))\n",
    "    model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "    embeddings = model.encode(docs, show_progress_bar=True)\n",
    "    umap_embeddings = umap.UMAP(\n",
    "        n_neighbors=15, n_components=5, metric='cosine', random_state=7, verbose=True\n",
    "    ).fit_transform(embeddings)\n",
    "    print('\\n Perform Clustering..')\n",
    "    cluster = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=2, metric='euclidean', cluster_selection_method='eom'\n",
    "    ).fit(umap_embeddings)\n",
    "    docs_df = pd.DataFrame(raw_text, columns=['doc'])\n",
    "    docs_df['topic'] = cluster.labels_\n",
    "    docs_df['doc_id'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['topic'], as_index=False).agg({'doc': ' '.join})\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.doc.values, m=len(raw_text))\n",
    "\n",
    "    for i in range(10):\n",
    "        similarities = cosine_similarity(tf_idf.T)\n",
    "        np.fill_diagonal(similarities, 0)\n",
    "\n",
    "        topic_sizes = (\n",
    "            docs_df.groupby(['topic'])\n",
    "            .count()\n",
    "            .sort_values('doc', ascending=False)\n",
    "            .reset_index()\n",
    "        )\n",
    "        topic_to_merge = topic_sizes.iloc[-1].topic\n",
    "        topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
    "\n",
    "        docs_df.loc[docs_df.topic == topic_to_merge, 'topic'] = topic_to_merge_into\n",
    "        old_topics = docs_df.sort_values(by='topic').topic.unique()\n",
    "        map_topics = {\n",
    "            old_topics: index - 1 for index, old_topics in enumerate(old_topics)\n",
    "        }\n",
    "        docs_df.topic = docs_df.topic.map(map_topics)\n",
    "        docs_per_topic = docs_df.groupby(['topic'], as_index=False).agg(\n",
    "            {'doc': ' '.join}\n",
    "        )\n",
    "\n",
    "        tf_idf, count = c_tf_idf(docs_per_topic.doc.values, len(raw_text))\n",
    "        top_n_words = extract_top_n_words_per_topic(\n",
    "            tf_idf, count, docs_per_topic['topic'], n=20\n",
    "        )\n",
    "\n",
    "    topic_sizes = (\n",
    "        docs_df.groupby(['topic'])\n",
    "        .doc.count()\n",
    "        .reset_index()\n",
    "        .rename({'doc': 'size'}, axis='columns')\n",
    "        .sort_values('size', ascending=False)\n",
    "    )\n",
    "\n",
    "    score = round(sum(topic_sizes['size'].values) / topic_sizes.shape[0], 4)\n",
    "    print(\n",
    "        '\\nA total of {} topic(s) found, with an average of {} tweet(s)'.format(\n",
    "            len(topic_sizes['topic']), score\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return topic_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes = topic_modelling(df=tweets_df, text_col='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e3425",
   "metadata": {},
   "source": [
    "### PLot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_z_scores = np.abs(stats.zscore(topic_sizes))\n",
    "topic_sizes_of = topic_sizes[(abs_z_scores < 3).all(axis=1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3), dpi=100)\n",
    "ax[0].bar(topic_sizes['topic'], topic_sizes['size'])\n",
    "ax[0].set_title('Without Normalization')\n",
    "ax[0].set_xlabel('N Topics')\n",
    "ax[0].set_ylabel('Sizes of Topics')\n",
    "ax[1].bar(topic_sizes_of['topic'], topic_sizes_of['size'])\n",
    "ax[1].axhline(y=topic_sizes_of['size'].median(), label='Median', color='red')\n",
    "ax[1].set_title('With Normalization')\n",
    "ax[1].set_xlabel('N Topics')\n",
    "ax[1].set_ylabel('Sizes of Topics')\n",
    "plt.legend()\n",
    "plt.suptitle('Topics Sizes Overview', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de572c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment Analysis<a id='sentiment'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f706539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(df, text_col: str) -> pd.DataFrame:\n",
    "    text_data = df[text_col].apply(lambda x: clean_the_text2(x))\n",
    "\n",
    "    sentiment_pipeline = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    bert_results = pd.DataFrame(sentiment_pipeline(list(text_data.values)))\n",
    "    sentiment_dict = {\n",
    "        'LABEL_0': 'Negative',\n",
    "        'LABEL_1': 'Neutral',\n",
    "        'LABEL_2': 'Positive',\n",
    "    }\n",
    "    bert_results['label'] = bert_results['label'].map(sentiment_dict)\n",
    "    bert_results.columns = ['bert_label', 'bert_score']\n",
    "\n",
    "    fair_results = list()\n",
    "    classifier = TextClassifier.load(\"en-sentiment\")\n",
    "    for sentence in text_data:\n",
    "        s = Sentence(sentence)\n",
    "        classifier.predict(s)\n",
    "        fair_results.append(str(s.labels)[1:-1].split(' '))\n",
    "\n",
    "    fair_results = pd.DataFrame(fair_results, columns=['label', 'score'])\n",
    "    fair_results['score'] = (\n",
    "        fair_results['score'].str.replace('(', '').str.replace(')', '')\n",
    "    )\n",
    "    fair_results.columns = ['flair_label', 'flair_score']\n",
    "\n",
    "    sentiments = pd.concat([df, bert_results, fair_results], axis=1)\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64640227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments = get_sentiment(df=tweets_df, text_col='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95b4c7",
   "metadata": {},
   "source": [
    "### PLot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 5), dpi=200)\n",
    "sentiments['bert_label'].value_counts().plot(\n",
    "    kind='barh', ax=ax[0, 0], title='Sentiment Distributions'\n",
    ")\n",
    "ax[0, 0].set_xlabel('Count')\n",
    "ax[0, 0].set_ylabel('Twitter-roBERTa-base Model')\n",
    "sentiments.groupby('bert_label').agg({'bert_score': 'median'}).plot(\n",
    "    kind='barh',\n",
    "    ax=ax[0, 1],\n",
    "    legend=False,\n",
    "    xlim=[0, 1],\n",
    "    title='Median score of sentiment class',\n",
    ")\n",
    "ax[0, 1].set_xlabel('Score')\n",
    "ax[0, 1].set_ylabel('Sentiment')\n",
    "\n",
    "sentiments['flair_label'].value_counts().plot(\n",
    "    kind='barh', ax=ax[1, 0], title='Sentiment Distributions'\n",
    ")\n",
    "ax[1, 0].set_xlabel('Count')\n",
    "ax[1, 0].set_ylabel('sentiment-curated-distilbert Model')\n",
    "sentiments.groupby('flair_label').agg({'flair_score': 'median'}).plot(\n",
    "    kind='barh',\n",
    "    ax=ax[1, 1],\n",
    "    legend=False,\n",
    "    xlim=[0, 1],\n",
    "    title='Median score of sentiment class',\n",
    ")\n",
    "ax[1, 1].set_xlabel('Score')\n",
    "ax[1, 1].set_ylabel('Sentiment')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b4989",
   "metadata": {},
   "source": [
    "## WordCloud of Named Entity Recognition<a id='wcentity'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_entity(df, text_col: str) -> None:\n",
    "    print('Preparing model...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "    ner_model = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    ner_list = list()\n",
    "\n",
    "    print('Extracting...')\n",
    "    for line in tqdm(list(df[text_col].values)):\n",
    "        ner_results = ner_model(line)\n",
    "        selected_entity = ['B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "        results_entity = [\n",
    "            entity\n",
    "            for entity in ner_results\n",
    "            if entity.get('entity') in (selected_entity)\n",
    "        ]\n",
    "        results_word = [entity.get('word') for entity in results_entity]\n",
    "        ner_list.extend(results_word)\n",
    "\n",
    "    print('Concatenating...')\n",
    "    single_line_ner = ''\n",
    "    for word in ner_list:\n",
    "        single_line_ner += word + ' '\n",
    "\n",
    "    print('Building word cloud...')\n",
    "    sws = stopwords.words('english') + list(string.punctuation)\n",
    "    wc = WordCloud(\n",
    "        stopwords=sws, background_color='white', width=1000, height=1000\n",
    "    ).generate(single_line_ner)\n",
    "\n",
    "    print('Plotting...')\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(wc)\n",
    "    plt.tight_layout()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed358f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frequent_entity(df=tweets_df, text_col='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d697a",
   "metadata": {},
   "source": [
    "# Dependencies<a id='dependencies'></a>\n",
    "\n",
    "[^1]: https://spacy.io/usage/linguistic-features#named-entities\n",
    "[^10]: https://hdbscan.readthedocs.io/en/latest/\n",
    "[^2]: https://csurfer.github.io/rake-nltk/_build/html/index.html\n",
    "[^3]: https://radimrehurek.com/gensim_3.8.3/summarization/keywords.html\n",
    "[^4]: https://github.com/LIAAD/yake\n",
    "[^5]: https://github.com/MaartenGr/KeyBERT\n",
    "[^6]: https://trends.google.com/trends/\n",
    "[^7]: https://spacy.io/api/matcher\n",
    "[^8]: https://github.com/UKPLab/sentence-transformers\n",
    "[^9]: https://github.com/lmcinnes/umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Google News Scraper\n",
    "# !pip install dateparser\n",
    "# !pip install feedparser==6.0.8\n",
    "# !pip install requests==2.26.0\n",
    "# !pip install bs4==0.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f800bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For plotting in network and bulk scraping\n",
    "# !pip install numpy==1.21.4\n",
    "# !pip install pandas==1.3.5\n",
    "# !pip install networkx==2.6.3\n",
    "# !pip install matplotlib==3.5.1\n",
    "# !pip install scikit-learn==1.0.1\n",
    "# !pip install spacy==3.2.1\n",
    "# !pip install langdetect\n",
    "# !pip install deep_translator\n",
    "# !pip install tqdm==4.62.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tweets related\n",
    "# !pip install tweepy==3.10.0\n",
    "# !pip install pandas==1.3.4\n",
    "# !pip install nltk==3.7\n",
    "# !pip install spacy=3.2.0\n",
    "# !pip install rake-nltk==1.0.6\n",
    "# !pip install gensim==3.8.3\n",
    "# !pip install yake==0.4.8\n",
    "# !pip install keybert==0.5.0\n",
    "# !pip install pytrends==4.8.0\n",
    "# !pip install tqdm==4.62.3\n",
    "# !pip install numpy==1.19.5\n",
    "# !pip install sklearn==0.23.2\n",
    "# !pip install sentence_transformers==2.2.0\n",
    "# !pip install umap==0.5.2\n",
    "# !pip install hdbscan==0.8.27\n",
    "# !pip install matplotlib==3.5.1\n",
    "# !pip install scipy==1.5.4\n",
    "# !pip install transformers==4.16.2\n",
    "# !pip install wordcloud==1.8.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
