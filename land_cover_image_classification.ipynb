{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a75a0860",
      "metadata": {
        "id": "a75a0860"
      },
      "source": [
        "# `Land Cover` Classification\n",
        "\n",
        "Data Source: EuroSAT : Land Use and Land Cover Classification with Sentinel-2 <br>\n",
        "[Download to local](https://madm.dfki.de/files/sentinel/EuroSAT.zip)\n",
        "\n",
        "- Anomaly detection with CVAE\n",
        "- Multi Convolutional Neural Network and Transfer Learning pretrained model classification\n",
        "\n",
        "[Citation](#citation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e35f33",
      "metadata": {
        "id": "50e35f33"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "import glob\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd63143e",
      "metadata": {
        "id": "dd63143e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05cc1cb",
      "metadata": {
        "id": "d05cc1cb"
      },
      "source": [
        "# Anomaly Detection with CVAE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce33add5",
      "metadata": {
        "id": "ce33add5"
      },
      "source": [
        "## CVAE Model Constructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531c8c65",
      "metadata": {
        "id": "531c8c65"
      },
      "outputs": [],
      "source": [
        "def encoder_block(encoder_inputs_shape, latent_dim):\n",
        "    inputs = tf.keras.Input(shape=encoder_inputs_shape, name='encode_input_layer')\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', name='encode_conv_layer_1')(inputs)\n",
        "    x = tf.keras.layers.LeakyReLU(name='encode_lrelu_1')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', name='encode_conv_layer_2')(x)\n",
        "    x = tf.keras.layers.LeakyReLU(name='encode_lrelu_2')(x)\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', name='encode_conv_layer_3')(x)\n",
        "    x = tf.keras.layers.LeakyReLU(name='encode_lrelu_3')(x)\n",
        "    x = tf.keras.layers.Flatten(name='encode_flatten_layer')(x)\n",
        "    mean = tf.keras.layers.Dense(latent_dim)(x)\n",
        "    log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
        "\n",
        "    class Sampling(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            mean, log_var = inputs\n",
        "            epsilon = tf.keras.backend.random_normal(\n",
        "                shape=(tf.keras.backend.shape(mean)[0], latent_dim),\n",
        "                mean=0.0,\n",
        "                stddev=0.1,\n",
        "            )\n",
        "            return mean + tf.keras.backend.exp(log_var) * epsilon\n",
        "\n",
        "    z = Sampling()([mean, log_var])\n",
        "    encoder = tf.keras.Model(\n",
        "        inputs=inputs, outputs=[mean, log_var, z], name='encoder_module'\n",
        "    )\n",
        "    return mean, log_var, inputs, encoder\n",
        "\n",
        "\n",
        "def decoder_block(decoder_inputs_shape):\n",
        "    inputs = tf.keras.Input(shape=(decoder_inputs_shape,), name='decode_input_layer')\n",
        "    x = tf.keras.layers.Dense(units=8 * 8 * 64, name='decode_dense_layer')(inputs)\n",
        "    x = tf.keras.layers.LeakyReLU(name='decode_lrelu_1')(x)\n",
        "    x = tf.keras.layers.Reshape(target_shape=(8, 8, 64), name='decode_reshape_layer')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=3,strides=2,padding='same',name='decode_conv2t_layer_1',)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(name='decode_lrelu_2')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=64,kernel_size=3,strides=2,padding='same',name='decode_conv2t_layer_2',)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(name='decode_lrelu_3')(x)\n",
        "    x = tf.keras.layers.Conv2DTranspose(filters=32,kernel_size=3,strides=2,padding='same',name='decode_conv2t_layer_3',)(x)\n",
        "    x = tf.keras.layers.LeakyReLU(name='decode_lrelu_4')(x)\n",
        "    outputs = tf.keras.layers.Conv2DTranspose(filters=3,kernel_size=3,activation='sigmoid',padding='same',name='decode_output_layer',)(x)\n",
        "    decoder = tf.keras.Model(inputs=inputs, outputs=outputs, name='decoder_module')\n",
        "    return outputs, decoder\n",
        "\n",
        "\n",
        "def create_model(input_shape, latent_dim):\n",
        "\n",
        "    mean, log_var, encoder_inputs, encoder = encoder_block(\n",
        "        encoder_inputs_shape=input_shape, latent_dim=latent_dim\n",
        "    )\n",
        "    decoder_outputs, decoder = decoder_block(decoder_inputs_shape=latent_dim)\n",
        "\n",
        "    # display(encoder.summary())\n",
        "    # display(decoder.summary())\n",
        "    model_outputs = decoder(encoder(encoder_inputs)[2])\n",
        "    vae = tf.keras.Model(inputs=encoder_inputs, outputs=model_outputs, name='vae_model')\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.reduce_sum(\n",
        "            tf.keras.losses.binary_crossentropy(encoder_inputs, model_outputs),\n",
        "            axis=(1, 2),\n",
        "        )\n",
        "    )\n",
        "    kl_loss = -0.5 * tf.keras.backend.sum(\n",
        "        1 + log_var - tf.keras.backend.square(mean) - tf.keras.backend.exp(log_var),\n",
        "        axis=1,\n",
        "    )\n",
        "    vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
        "    vae.add_loss(vae_loss)\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
        "    return vae"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b3c24f",
      "metadata": {
        "id": "91b3c24f"
      },
      "source": [
        "## Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872ecadd",
      "metadata": {
        "id": "872ecadd"
      },
      "outputs": [],
      "source": [
        "def plot_result(history, epochs):\n",
        "    loss = history.history['loss']\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 4))\n",
        "    ax.plot(epochs_range, loss, label='Training Loss')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Reconstruction + KL Loss')\n",
        "    ax.set_title('Training and Validation Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dedc535",
      "metadata": {
        "id": "0dedc535"
      },
      "outputs": [],
      "source": [
        "def test_cvae(model, image_generator):\n",
        "    images = next(iter(image_generator))[0]\n",
        "    for image in images:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        img_array = tf.keras.utils.img_to_array(image)\n",
        "        img_array = img_array.astype('float32')\n",
        "        img_array = tf.expand_dims(img_array, 0)\n",
        "        reconstructed_image = model.predict(img_array, steps=1)[0]\n",
        "        ax[0].imshow(image)\n",
        "        ax[1].imshow(reconstructed_image)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8830cd78",
      "metadata": {
        "id": "8830cd78"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a0d77c",
      "metadata": {
        "id": "f8a0d77c"
      },
      "outputs": [],
      "source": [
        "DATASET_DIR = '/content/drive/MyDrive/land_cover_datasets/2750/'\n",
        "DATASET_DIR_DEPTH = '/content/drive/MyDrive/land_cover_datasets/2750/*/*'\n",
        "IMAGE_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "LATENT_DIM = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a96a62",
      "metadata": {
        "id": "85a96a62"
      },
      "outputs": [],
      "source": [
        "def get_class(path):\n",
        "    return path.split('/', 6)[-1]\n",
        "\n",
        "def get_id(path):\n",
        "    return path.split('/')[6]\n",
        "\n",
        "image_df = pd.DataFrame(glob.glob(DATASET_DIR_DEPTH), columns=['path'])\n",
        "image_df['id'] = image_df['path'].apply(get_class)\n",
        "image_df['class'] = image_df['path'].apply(get_id)\n",
        "display(image_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa91960d",
      "metadata": {
        "id": "fa91960d"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32e3f63b",
      "metadata": {
        "id": "32e3f63b"
      },
      "outputs": [],
      "source": [
        "def anomaly_detection(image_df, class_label):\n",
        "\n",
        "    print('\\nCreating model...')\n",
        "    model = create_model(input_shape=INPUT_SHAPE, latent_dim=LATENT_DIM)\n",
        "\n",
        "    subset_df = image_df[image_df['class'] == class_label]\n",
        "\n",
        "    image_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        horizontal_flip=True, vertical_flip=True, rotation_range=0.2, rescale=1.0 / 255\n",
        "    )\n",
        "\n",
        "    print('Processing image...')\n",
        "    image_generator = image_gen.flow_from_dataframe(\n",
        "        dataframe=subset_df,\n",
        "        directory=DATASET_DIR,\n",
        "        x_col='id',\n",
        "        y_col='class',\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        class_mode='input',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        seed=7,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    print('Training model...')\n",
        "    history = model.fit(image_generator, epochs=EPOCHS, verbose=2)\n",
        "\n",
        "    print('PLotting result...')\n",
        "    plot_result(history=history, epochs=EPOCHS)\n",
        "\n",
        "    total_step = len(image_generator)\n",
        "    res = model.predict(image_generator)\n",
        "    upb = np.median(res) + np.std(res)  # np.percentile(a=res, q=90)\n",
        "    lwb = np.median(res) - np.std(res)  # np.percentile(a=res, q=10)\n",
        "\n",
        "    print('Anomaly detecting...')\n",
        "    for steps, images in enumerate(image_generator):\n",
        "        images = images[0]\n",
        "\n",
        "        idx = (image_generator.batch_index - 1) * image_generator.batch_size\n",
        "        current_batch_file_list = image_generator.filenames[\n",
        "            idx : idx + image_generator.batch_size\n",
        "        ]\n",
        "\n",
        "        if len(current_batch_file_list) != 0:\n",
        "            outliers_image_file_list = list()\n",
        "\n",
        "            for idx, image in enumerate(images):\n",
        "                # fig, ax = plt.subplots(figsize=(10,5))\n",
        "                img_array = tf.keras.utils.img_to_array(image)\n",
        "                img_array = img_array.astype('float32')\n",
        "                img_array = tf.expand_dims(img_array, 0)\n",
        "                boundary = model.predict(img_array)\n",
        "                if not lwb <= np.median(boundary) <= upb:\n",
        "                    outliers_image_file_list.append(current_batch_file_list[idx])\n",
        "                # ax.imshow(img_array[0])\n",
        "                # ax.set_title(f'{current_batch_file_list[idx]}')\n",
        "                # plt.show()\n",
        "\n",
        "        if steps > total_step:\n",
        "            break\n",
        "\n",
        "    return outliers_image_file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e494af46",
      "metadata": {
        "id": "e494af46"
      },
      "outputs": [],
      "source": [
        "anomaly_list = list()\n",
        "\n",
        "for class_label in image_df['class'].unique():\n",
        "    anomaly_list.extend(anomaly_detection(image_df=image_df, class_label=class_label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c30d4c8c",
      "metadata": {
        "id": "c30d4c8c"
      },
      "outputs": [],
      "source": [
        "ncols = 5\n",
        "nrows = int(len(anomaly_list) / ncols)\n",
        "axes = []\n",
        "fig = plt.figure(figsize=(50, 50))\n",
        "\n",
        "for i in range(nrows * ncols):\n",
        "    axes.append(fig.add_subplot(nrows, ncols, i + 1))\n",
        "    axes[-1].set_title(f'{str(anomaly_list[i])}')\n",
        "    img_array = tf.keras.preprocessing.image.load_img(DATASET_DIR + anomaly_list[i])\n",
        "    img_array = tf.keras.utils.img_to_array(img_array)\n",
        "    img_array = img_array.astype('float32')\n",
        "    plt.imshow(img_array.astype(np.uint8))\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f18b5c",
      "metadata": {
        "id": "39f18b5c"
      },
      "outputs": [],
      "source": [
        "anomaly_path = os.getcwd() + \"/anomaly_/\"\n",
        "\n",
        "if not os.path.exists(anomaly_path):\n",
        "    os.makedirs(anomaly_path)\n",
        "\n",
        "try:\n",
        "    for file in anomaly_list:\n",
        "        from_ = DATASET_DIR + file\n",
        "        to_ = anomaly_path + file.split('/')[-1]\n",
        "        os.replace(from_, to_)\n",
        "except:\n",
        "    print('File not found or moved previously')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16d1dfca",
      "metadata": {
        "id": "16d1dfca"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99b9685",
      "metadata": {
        "id": "b99b9685"
      },
      "source": [
        "## Multi Convolutional Neural Network(MCNN) Model Building Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd13c471",
      "metadata": {
        "id": "bd13c471"
      },
      "outputs": [],
      "source": [
        "def augmentation_layer(x):\n",
        "    x = tf.keras.layers.RandomFlip('horizontal')(x)\n",
        "    x = tf.keras.layers.RandomRotation(0.1)(x)\n",
        "    x = tf.keras.layers.RandomZoom(0.1)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def mlp(x, filters, idx):\n",
        "    x = tf.keras.layers.Conv2D(filters=filters, kernel_size=3, padding='same', name=f'conv_layer_{idx}')(x)\n",
        "    x = tf.keras.layers.Activation('relu', name=f'relu_{idx}')(x)\n",
        "    # x = tf.keras.layers.BatchNormalization(name=f'normalization_layer_{idx}')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), padding='valid', name=f'pooling_layer_{idx}')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2, name=f'dropout_layer_{idx}')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def create_mcnn_model(input_shape, num_classes, augmentation):\n",
        "    inputs = tf.keras.Input(shape=input_shape, name='input_layer')\n",
        "    if augmentation:\n",
        "        x = augmentation_layer(inputs)\n",
        "        x = tf.keras.layers.Rescaling(scale=1.0 / 255, name='rescaling_layer')(x)\n",
        "    else:\n",
        "        x = tf.keras.layers.Rescaling(scale=1.0 / 255, name='rescaling_layer')(inputs)\n",
        "    for idx, f in enumerate([16, 32, 64, 64]):\n",
        "        x = mlp(x, f, idx + 1)\n",
        "    x = tf.keras.layers.Flatten(name='flatten_layer')(x)\n",
        "    x = tf.keras.layers.Dense(units=128, activation='relu', name='dense_layer')(x)\n",
        "    outputs = tf.keras.layers.Dense(units=num_classes, name='output_layer')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mcnn_model')\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_mcnn_model(input_shape, num_classes, augmentation, train_ds, val_ds, epochs):\n",
        "\n",
        "    model = create_mcnn_model(\n",
        "        input_shape=input_shape, num_classes=num_classes, augmentation=augmentation\n",
        "    )\n",
        "    \n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch < tf.math.ceil(epochs/2):\n",
        "            return lr\n",
        "        else:\n",
        "            return lr * tf.math.exp(-0.1)\n",
        "    lr_callback_ = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    early_stopping_ = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0.001, patience=tf.math.ceil(epochs*0.5), restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/MCNN_checkpoint/cp-{epoch:04d}.ckpt\"\n",
        "    model_checkpoint_ = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath, save_weights_only=True, monitor='val_accuracy'\n",
        "    )\n",
        "    \n",
        "    all_callbacks = [lr_callback_, early_stopping_, model_checkpoint_]\n",
        "\n",
        "    model_history = model.fit(\n",
        "        train_ds, validation_data=val_ds, epochs=epochs, callbacks=all_callbacks\n",
        "    )\n",
        "\n",
        "    model.save(\"/MCNN_model/MCNN_model.h5\", save_format='h5')\n",
        "\n",
        "    return model, model_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625f9291",
      "metadata": {
        "id": "625f9291"
      },
      "source": [
        "## Transfer Learning pretrained model(TLPM) Model Building Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1fc71c",
      "metadata": {
        "id": "ab1fc71c"
      },
      "outputs": [],
      "source": [
        "def create_tlpm_model(input_shape, num_classes):\n",
        "    base_model = tf.keras.applications.NASNetMobile(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "\n",
        "    fine_tune_threshold = int(len(base_model.layers) * 0.8)\n",
        "    for layer in base_model.layers[:fine_tune_threshold]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(shape=input_shape, name='input_layer')\n",
        "    x = tf.keras.applications.nasnet.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D(name='global_avg_pool_layer')(x)\n",
        "    x = tf.keras.layers.Dropout(rate=0.2, name='dropout_layer')(x)\n",
        "    outputs = tf.keras.layers.Dense(units=num_classes, name='dense_layer')(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='tl_model')\n",
        "    base_learning_rate = 0.0001\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_tlpm_model(input_shape, num_classes, train_ds, val_ds, epochs):\n",
        "\n",
        "    model = create_tlpm_model(input_shape=input_shape, num_classes=num_classes)\n",
        "\n",
        "    early_stopping_ = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0.01,\n",
        "        patience=epochs // 3,\n",
        "        restore_best_weights=True,\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/TLPM_checkpoint/cp-{epoch:04d}.ckpt\"\n",
        "    model_checkpoint_ = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath, save_weights_only=True, monitor='val_accuracy'\n",
        "    )\n",
        "    all_callbacks = [early_stopping_, model_checkpoint_]\n",
        "\n",
        "    model_history = model.fit(\n",
        "        train_ds, validation_data=val_ds, epochs=epochs, callbacks=all_callbacks\n",
        "    )\n",
        "\n",
        "    model.save(\"/TLPM_model/TLPM_model.h5\", save_format='h5')\n",
        "\n",
        "    return model, model_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb83c791",
      "metadata": {
        "id": "fb83c791"
      },
      "source": [
        "## Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0ee434",
      "metadata": {
        "id": "0f0ee434"
      },
      "outputs": [],
      "source": [
        "def show_samples(ds):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for images, labels in ds.take(1):\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype('uint8'))\n",
        "            plt.title(CLASS_NAMES[labels[i]])\n",
        "            plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26637beb",
      "metadata": {
        "id": "26637beb"
      },
      "outputs": [],
      "source": [
        "def show_predict(model, ds, return_labels=False):\n",
        "    actual_label_list, predict_label_list = [], []\n",
        "    for images, labels in ds.take(1):\n",
        "        predicted = model.predict(images)\n",
        "        for i in range(BATCH_SIZE):\n",
        "            actual = labels[i].numpy()\n",
        "            actual_label_list.append(actual)\n",
        "            predict = predicted[i].argmax()\n",
        "            predict_label_list.append(predict)\n",
        "            print(f'Actual: {CLASS_NAMES[actual]}')\n",
        "            print(f'Predicted: {CLASS_NAMES[predict]}')\n",
        "            plt.imshow(images[i].numpy().astype('uint8'))\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "    if return_labels:\n",
        "        return actual_label_list, predict_label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c10478",
      "metadata": {
        "id": "13c10478"
      },
      "outputs": [],
      "source": [
        "def plot_results(history, epochs, title):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    ax[0].plot(epochs_range, acc, label='Training Accuracy')\n",
        "    ax[0].plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    ax[0].legend(loc='lower right')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_title('Training and Validation Accuracy')\n",
        "    ax[1].plot(epochs_range, loss, label='Training Loss')\n",
        "    ax[1].plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    ax[1].legend(loc='upper right')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_title(f'Training and Validation Loss_{title}')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9136ac4f",
      "metadata": {
        "id": "9136ac4f"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5441d5",
      "metadata": {
        "id": "0f5441d5"
      },
      "outputs": [],
      "source": [
        "# Option 1: Tf.data.dataset, Memory leak possible\n",
        "def loading_data_1(\n",
        "    dataset_dir, total_size, train_size, image_size, batch_size, class_names\n",
        "):\n",
        "\n",
        "    list_ds = tf.data.Dataset.list_files(dataset_dir, shuffle=False)\n",
        "    list_ds = list_ds.shuffle(total_size, reshuffle_each_iteration=False)\n",
        "\n",
        "    train_ds = list_ds.take(train_size)\n",
        "    val_ds = list_ds.skip(train_size)\n",
        "\n",
        "    def parse_image(image_path):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.io.decode_jpeg(image)\n",
        "        image = tf.image.resize(image, [image_size, image_size])\n",
        "        label = tf.strings.split(image_path, os.sep)\n",
        "        one_hot = label[-2] == class_names\n",
        "        return image, tf.argmax(one_hot)\n",
        "\n",
        "    train_ds = train_ds.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
        "    val_ds = val_ds.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    def configure_for_performance(ds):\n",
        "        ds = ds.cache()\n",
        "        ds = ds.shuffle(buffer_size=len(ds))\n",
        "        ds = ds.batch(batch_size)\n",
        "        ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    train_ds = configure_for_performance(train_ds)\n",
        "    val_ds = configure_for_performance(val_ds)\n",
        "\n",
        "    print(f\"Total size: {len(list_ds)}\")\n",
        "    print(f\"Train size: {len(train_ds)*batch_size}\")\n",
        "    print(f\"Validation size: {len(val_ds)*batch_size}\")\n",
        "\n",
        "    return train_ds, val_ds\n",
        "\n",
        "\n",
        "# Option 2: Tf.keras.utils\n",
        "def loading_data_2(dataset_dir, batch_size, image_size):\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        shuffle=True,\n",
        "        seed=7,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(image_size, image_size),\n",
        "    )\n",
        "\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_dir,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        shuffle=True,\n",
        "        seed=7,\n",
        "        batch_size=batch_size,\n",
        "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    )\n",
        "\n",
        "    class_names = train_ds.class_names\n",
        "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f2c1ae",
      "metadata": {
        "id": "f8f2c1ae"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "loading_data_option = 2\n",
        "\n",
        "if loading_data_option == 1:\n",
        "    DATASET_DIR = \"/content/drive/MyDrive/land_cover_datasets/2750/*/*\"\n",
        "    CLASS_DIR = \"/content/drive/MyDrive/land_cover_datasets/2750/\"\n",
        "    TOTAL_SIZE = len(glob.glob(DATASET_DIR))\n",
        "    TRAIN_SIZE = int(TOTAL_SIZE * 0.8)\n",
        "    CLASS_NAMES = os.listdir(CLASS_DIR)\n",
        "    train_ds, val_ds = loading_data_1(\n",
        "        dataset_dir=DATASET_DIR,\n",
        "        total_size=TOTAL_SIZE,\n",
        "        train_size=TRAIN_SIZE,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_names=CLASS_NAMES,\n",
        "    )\n",
        "elif loading_data_option == 2:\n",
        "    DATASET_DIR = \"/content/drive/MyDrive/land_cover_datasets/2750/\"\n",
        "    train_ds, val_ds, CLASS_NAMES = loading_data_2(\n",
        "        dataset_dir=DATASET_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8286604",
      "metadata": {
        "id": "a8286604"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31aef5b",
      "metadata": {
        "id": "c31aef5b"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "NUM_CLASSES = 10\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6cd5058",
      "metadata": {
        "id": "e6cd5058"
      },
      "outputs": [],
      "source": [
        "MCNN, MCNN_history = train_mcnn_model(\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    augmentation=True,\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=EPOCHS,\n",
        ")\n",
        "\n",
        "plot_results(\n",
        "    history=MCNN_history, epochs=EPOCHS, title='Multi Convolutional Neural Network'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cddfff",
      "metadata": {
        "id": "36cddfff"
      },
      "outputs": [],
      "source": [
        "TLPM, TLPM_history = train_tlpm_model(\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=EPOCHS,\n",
        ")\n",
        "\n",
        "plot_results(\n",
        "    history=TLPM_history, epochs=EPOCHS, title='Transfer Learning with Pretrained Model'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babbd58b",
      "metadata": {
        "id": "babbd58b"
      },
      "source": [
        "## Predicting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac3819e",
      "metadata": {
        "id": "2ac3819e"
      },
      "outputs": [],
      "source": [
        "loaded_MCNN = tf.keras.models.load_model(\"/MCNN_model/MCNN_model.h5\")\n",
        "loaded_TLPM = tf.keras.models.load_model(\"/TLPM_model/TLPM_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd22d11",
      "metadata": {
        "id": "6bd22d11"
      },
      "outputs": [],
      "source": [
        "actual_label_list, predict_label_list = show_predict(\n",
        "    model=loaded_MCNN, ds=val_ds, return_labels=True\n",
        ")\n",
        "tf.math.confusion_matrix(actual_label_list, predict_label_list, num_classes=NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b01ec63",
      "metadata": {
        "id": "1b01ec63"
      },
      "outputs": [],
      "source": [
        "actual_label_list, predict_label_list = show_predict(\n",
        "    model=loaded_TLPM, ds=val_ds, return_labels=True\n",
        ")\n",
        "tf.math.confusion_matrix(actual_label_list, predict_label_list, num_classes=NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e04a429",
      "metadata": {
        "id": "8e04a429"
      },
      "source": [
        "# Citation<a id='citation'></a>\n",
        "\n",
        "@article{helber2019eurosat,<br>\n",
        "\n",
        "- title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},<br>\n",
        "- author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},<br>\n",
        "- journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},<br>\n",
        "- year={2019},<br>\n",
        "- publisher={IEEE}<br>\n",
        "  }<br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d05cc1cb"
      ],
      "name": "land_cover_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}