{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4fd6fc0",
   "metadata": {
    "id": "PLzF3xX8Y3oj"
   },
   "source": [
    "# Chess Pieces Images Classification\n",
    "\n",
    "Abstract: \n",
    "\n",
    "Deep learning has been identified as the state of the art model in terms of overall predictive accuracy with large datasets, particularly in image classification tasks. An abundance of models and techniques has been proposed previously and achieved promising results and the accuracy of this model tends to increase symmetrically with the number of datasets. The problem of lack of dataset is one of the limitations of the status quo of image classification. In this study, a small dataset is prepared taken from online sources is used to train three different models and comparison, however, what constitutes a good model architecture and techniques to be used for the cases having a small dataset had been the main concern in this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1071ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmMtuvUwbSFd",
    "outputId": "27d2dc10-baa0-4a46-bcdc-3146a900e6d1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883b535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmHeao0IbTy7",
    "outputId": "f5b33a78-4cba-411b-f2ac-2f5f4c8d4011"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897ec29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CDU2BqB5yh6",
    "outputId": "5e452028-0f40-4131-b87f-bc0fea886eb5"
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"/content/drive/MyDrive/Chess\"\n",
    "for pieces in os.listdir(DATASET_DIR):\n",
    "    print(pieces)\n",
    "    print(len(os.listdir(DATASET_DIR + \"/\" + pieces)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e39219",
   "metadata": {
    "id": "5XWFzvSoUrKZ"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae3d65",
   "metadata": {
    "id": "hYGYbJZV-KeN"
   },
   "outputs": [],
   "source": [
    "def show_samples(ds):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in ds.take(1):\n",
    "        for i in range(16):\n",
    "            ax = plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(CLASS_NAMES[labels[i]])\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002db28c",
   "metadata": {
    "id": "rJI6HOoZ-MVh"
   },
   "outputs": [],
   "source": [
    "def show_predict(model, ds, return_labels=False):\n",
    "    actual_label_list, predict_label_list = [], []\n",
    "    for images, labels in ds.take(4):\n",
    "        predicted = model.predict(images)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            actual = labels[i].numpy()\n",
    "            actual_label_list.append(actual)\n",
    "            predict = predicted[i].argmax()\n",
    "            predict_label_list.append(predict)\n",
    "            print(f\"Actual: {CLASS_NAMES[actual]}\")\n",
    "            print(f\"Predicted: {CLASS_NAMES[predict]}\")\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "    if return_labels:\n",
    "        return actual_label_list, predict_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c58f3",
   "metadata": {
    "id": "3l0ObC1j-NxZ"
   },
   "outputs": [],
   "source": [
    "def plot_results(history, epochs, title):\n",
    "    acc = history.history[\"accuracy\"]\n",
    "    val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4), dpi=150)\n",
    "    ax[0].plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "    ax[0].plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_title(\"Training and Validation Accuracy\")\n",
    "    ax[1].plot(epochs_range, loss, label=\"Training Loss\")\n",
    "    ax[1].plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_title(\"Training and Validation Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cdb6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXqSLGatZ5P4",
    "outputId": "ae250c39-866f-483f-f5f2-909616042a23"
   },
   "outputs": [],
   "source": [
    "def loading_data(dataset_dir, batch_size, image_size):\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        shuffle=True,\n",
    "        seed=7,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(image_size, image_size),\n",
    "    )\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        dataset_dir,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        shuffle=True,\n",
    "        seed=7,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, class_names\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Chess\"\n",
    "train_ds, val_ds, CLASS_NAMES = loading_data(\n",
    "    dataset_dir=DATASET_DIR, batch_size=BATCH_SIZE, image_size=IMAGE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c485011",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "jXgzjxlpUjS3",
    "outputId": "4a579ed4-bc9d-4345-c345-0dc4cc759f34"
   },
   "outputs": [],
   "source": [
    "show_samples(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495b14d",
   "metadata": {
    "id": "vQhHXOFzYwm0"
   },
   "source": [
    "## MCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863ccf7",
   "metadata": {
    "id": "QirN3vlkZ5SK"
   },
   "outputs": [],
   "source": [
    "def augmentation_layer(x):\n",
    "    x = tf.keras.layers.RandomFlip(\"horizontal\")(x)\n",
    "    x = tf.keras.layers.RandomRotation(0.1)(x)\n",
    "    x = tf.keras.layers.RandomZoom(0.1)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def mlp(x, filters, idx):\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=filters, kernel_size=3, padding=\"same\", name=f\"conv_layer_{idx}\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\", name=f\"relu_{idx}\")(x)\n",
    "    # x = tf.keras.layers.BatchNormalization(name=f'normalization_layer_{idx}')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(\n",
    "        pool_size=(2, 2), padding=\"valid\", name=f\"pooling_layer_{idx}\"\n",
    "    )(x)\n",
    "    # x = tf.keras.layers.Dropout(0.2, name=f'dropout_layer_{idx}')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_mcnn_model(input_shape, num_classes, augmentation):\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_layer\")\n",
    "    if augmentation:\n",
    "        x = augmentation_layer(inputs)\n",
    "        x = tf.keras.layers.Rescaling(scale=1.0 / 255, name=\"rescaling_layer\")(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Rescaling(scale=1.0 / 255, name=\"rescaling_layer\")(inputs)\n",
    "    for idx, f in enumerate([16, 32, 64, 64, 64, 64, 64]):\n",
    "        x = mlp(x, f, idx + 1)\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten_layer\")(x)\n",
    "    x = tf.keras.layers.Dense(units=128, activation=\"relu\", name=\"dense_layer\")(x)\n",
    "    outputs = tf.keras.layers.Dense(units=num_classes, name=\"output_layer\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"mcnn_model\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_mcnn_model(input_shape, num_classes, augmentation, train_ds, val_ds, epochs):\n",
    "\n",
    "    model = create_mcnn_model(\n",
    "        input_shape=input_shape, num_classes=num_classes, augmentation=augmentation\n",
    "    )\n",
    "\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < tf.math.ceil(epochs / 2):\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    lr_callback_ = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    early_stopping_ = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=tf.math.ceil(epochs * 0.3),\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/MCNN_checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "    model_checkpoint_ = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath, save_weights_only=True, monitor=\"val_accuracy\"\n",
    "    )\n",
    "\n",
    "    all_callbacks = [early_stopping_, model_checkpoint_]  # lr_callback_\n",
    "\n",
    "    model_history = model.fit(\n",
    "        train_ds, validation_data=val_ds, epochs=epochs, callbacks=all_callbacks\n",
    "    )\n",
    "\n",
    "    # model.save(\"/MCNN_model/MCNN_model.h5\", save_format='h5')\n",
    "\n",
    "    return model, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570d792",
   "metadata": {
    "id": "meVcscBnZ5Ue"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "NUM_CLASSES = 6\n",
    "EPOCHS = 100\n",
    "\n",
    "model = create_mcnn_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES, augmentation=True)\n",
    "tf.keras.utils.plot_model(model, to_file='mcnn_model_architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7263a7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1CDvy9GcPS-",
    "outputId": "7a7e0262-eccd-4fdd-82a4-8f25e39bffdc"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)\n",
    "MCNN, MCNN_history = train_mcnn_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    augmentation=True,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d93b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "WK8p6im7K7OH",
    "outputId": "be8a8948-e35e-46d9-9474-025837b2bc01"
   },
   "outputs": [],
   "source": [
    "pd_MCNN_history = pd.DataFrame(MCNN_history.history)\n",
    "plot_results(\n",
    "    history=MCNN_history,\n",
    "    epochs=pd_MCNN_history.shape[0],\n",
    "    title=\"Multi Convolutional Neural Network\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04926cde",
   "metadata": {
    "id": "vj8oqukscq6D"
   },
   "outputs": [],
   "source": [
    "pd_MCNN_history.to_csv(\"pd_MCNN_history.csv\", index=False)\n",
    "MCNN.save(\"MCNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34261a4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ziv74Jvbq8lN",
    "outputId": "81bdef47-9669-4965-e7d1-7e42fc8e9590"
   },
   "outputs": [],
   "source": [
    "actual_label_list, predict_label_list = show_predict(\n",
    "    model=MCNN, ds=val_ds, return_labels=True\n",
    ")\n",
    "tf.math.confusion_matrix(actual_label_list, predict_label_list, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4637b0",
   "metadata": {
    "id": "vDu1hzOxYyYW"
   },
   "source": [
    "## TLPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebdc0f",
   "metadata": {
    "id": "P__9ePAycUC3"
   },
   "outputs": [],
   "source": [
    "def create_tlpm_model(input_shape, num_classes):\n",
    "    base_model = tf.keras.applications.NASNetMobile(\n",
    "        input_shape=input_shape, include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    fine_tune_threshold = int(len(base_model.layers) * 0.4)\n",
    "    for layer in base_model.layers[:fine_tune_threshold]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_layer\")\n",
    "    x = tf.keras.applications.nasnet.preprocess_input(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_avg_pool_layer\")(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2, name=\"dropout_layer\")(x)\n",
    "    outputs = tf.keras.layers.Dense(units=num_classes, name=\"dense_layer\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"tlpm_model\")\n",
    "    base_learning_rate = 0.0001\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_tlpm_model(input_shape, num_classes, train_ds, val_ds, epochs):\n",
    "\n",
    "    model = create_tlpm_model(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < tf.math.ceil(epochs / 2):\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    lr_callback_ = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    early_stopping_ = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0.001,\n",
    "        patience=tf.math.ceil(epochs * 0.5),\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"/TLPM_checkpoint/cp-{epoch:04d}.ckpt\"\n",
    "    model_checkpoint_ = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath, save_weights_only=True, monitor=\"val_accuracy\"\n",
    "    )\n",
    "    all_callbacks = [early_stopping_, model_checkpoint_]\n",
    "\n",
    "    model_history = model.fit(\n",
    "        train_ds, validation_data=val_ds, epochs=epochs, callbacks=all_callbacks\n",
    "    )\n",
    "\n",
    "    model.save(\"/TLPM_model/TLPM_model.h5\", save_format=\"h5\")\n",
    "\n",
    "    return model, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd7e14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 754
    },
    "id": "7uz1xOhtqKS4",
    "outputId": "76819500-e3e3-45ab-ca51-a14f12005d03"
   },
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "NUM_CLASSES = 6\n",
    "EPOCHS = 100\n",
    "\n",
    "model = create_tlpm_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "tf.keras.utils.plot_model(\n",
    "    model, to_file=\"mcnn_model_architecture.png\", show_shapes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ea1c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt1DomdncyYL",
    "outputId": "8456b592-80a0-46c2-da26-9a138b02ec89"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)\n",
    "TLPM, TLPM_history = train_tlpm_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd6a31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "5ZkWszLH4u9v",
    "outputId": "1faf5025-e96e-456a-ba7b-acc85ed66dac"
   },
   "outputs": [],
   "source": [
    "pd_TLPM_history = pd.DataFrame(TLPM_history.history)\n",
    "plot_results(\n",
    "    history=TLPM_history,\n",
    "    epochs=pd_TLPM_history.shape[0],\n",
    "    title=\"Transfer Learning with Pretrained Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7da7bb",
   "metadata": {
    "id": "g0Z0KY3mqijc"
   },
   "outputs": [],
   "source": [
    "pd_TLPM_history.to_csv(\"pd_TLPM_history.csv\", index=False)\n",
    "TLPM.save(\"TLPM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cc145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZDw7xrsYqrFR",
    "outputId": "2ed3ae56-ca6a-425c-ff9f-32319ec342e2"
   },
   "outputs": [],
   "source": [
    "actual_label_list, predict_label_list = show_predict(\n",
    "    model=TLPM, ds=val_ds, return_labels=True\n",
    ")\n",
    "tf.math.confusion_matrix(actual_label_list, predict_label_list, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395f4117",
   "metadata": {
    "id": "QO4gc6azV6hh"
   },
   "source": [
    "## Test and Compare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b78101",
   "metadata": {
    "id": "BLhVZgC3XiyF"
   },
   "outputs": [],
   "source": [
    "MCNN_model = tf.keras.models.load_model(\"/content/MCNN.h5\")\n",
    "TLPM_model = tf.keras.models.load_model(\"/content/TLPM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c156cf",
   "metadata": {
    "id": "KHqmgDiHYE9Q"
   },
   "outputs": [],
   "source": [
    "model_list = [MCNN_model, TLPM_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ec8da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0hJIoOiAUOPX",
    "outputId": "8a084202-cfc0-46f7-ed4e-6ca19686f9d5"
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "DATASET_DIR = \"/content/drive/MyDrive/Chess\"\n",
    "for pieces in tqdm.tqdm(os.listdir(DATASET_DIR)):\n",
    "    selected_img = random.sample(os.listdir(DATASET_DIR + \"/\" + pieces), n)\n",
    "    selected_img = [DATASET_DIR + \"/\" + pieces + \"/\" + img for img in selected_img]\n",
    "\n",
    "    for image in selected_img:\n",
    "        img = tf.keras.utils.load_img(image, target_size=(224, 224))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0)\n",
    "        predictions_list = list()\n",
    "        for model in model_list:\n",
    "            predictions = CLASS_NAMES[model.predict(img_array).argmax()]\n",
    "            predictions_list.append(predictions)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Model MCNN Predicted:\", predictions_list[0])\n",
    "        print(\"Model TLPM Predicted:\", predictions_list[1])\n",
    "        print(\"Actual Label: \", pieces)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chess Pieces Image Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
